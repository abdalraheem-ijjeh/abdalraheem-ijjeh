%\PassOptionsToPackage{draft}{graphicx}
\documentclass[10pt,aspectratio=169,dvipsnames]{beamer} % aspect ratio 16:9
%\graphicspath{{../../figures/}}

%\includeonlyframes{frame1,frame2,frame3}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\documentclass{beamer}
%\usepackage{geometry}
%\geometry{a4paper, portrait}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Packages
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{appendixnumberbeamer}
\usepackage{booktabs}
\usepackage{csvsimple} % for csv read
\usepackage{pgfplots}
\usepackage{xspace}
\usepackage{totcount}
\usepackage{tikz}
\usepackage{bm}
\usepackage{float}
\usepackage{eso-pic} 
\usepackage{wrapfig}
\usepackage{animate,media9}
\usepackage{subfig}
\usepackage{fancybox}
\usepackage{dashbox}
\usepackage{tcolorbox}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{xcolor}
\usepackage[document]{ragged2e}
\usepackage{caption}
\usepackage{comment}
\usepackage{mathtools}% Loads amsmath
\usepackage{movie15}
%\usepackage{subfigure}
\usepackage{beamerthemesplit}

%\usepackage[export]{adjustbox}
%\usepackage{background}
%\backgroundsetup{contents=preliminary,placement=bottom,color=blue}
%\usepackage{FiraSans}

%\usepackage{comment}
%\usetikzlibrary{external} % speedup compilation
%\tikzexternalize % activate!
%\usetikzlibrary{shapes,arrows} 

%\usepackage{bibentry}
%\nobibliography*
\usepackage{ifthen}
\newcounter{angle}
\setcounter{angle}{0}
%\usepackage{bibentry}
%\nobibliography*
\usepackage{caption}%

\graphicspath{{figures/}}

\captionsetup[figure]{labelformat=empty}%
\usefonttheme{structurebold}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Metropolis theme custom modification file
\input{metropolis_mods.tex}
%\usefonttheme[onlymath]{Serif} % It should be uncommented if Fira fonts in 
%%math does not work

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Custom commands
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% matrix command 
\newcommand{\matr}[1]{\mathbf{#1}} % bold upright (Elsevier, Springer)
%\newcommand{\matr}[1]{#1}   % pure math version
%\newcommand{\matr}[1]{\bm{#1}}  % ISO complying version
% vector command 
\newcommand{\vect}[1]{\mathbf{#1}} % bold upright (Elsevier, Springer)
% bold symbol
\newcommand{\bs}[1]{\boldsymbol{#1}}
% derivative upright command
\DeclareRobustCommand*{\drv}{\mathop{}\!\mathrm{d}}
\newcommand{\ud}{\mathrm{d}}
% 
\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}

%\usepackage{pgfpages}
%\setbeameroption{show only notes}
%\setbeameroption{show notes on second screen=left}
%\setbeamertemplate{note page}{\insertnote}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title page options
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \date{\today}
\date{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% option 1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{AN INTRODUCTION FOR ARTIFICIAL INTELLIGENCE APPROACHES IN SHM/NDT APPLICATIONS}

% Background for the first frame
\setbeamertemplate{background}{%
	\ifnum\value{framenumber}=0%
	\includegraphics[width=\paperwidth,height=\paperheight]{AI_BG_2.jpg}%
	\fi%
}


\subtitle{}
\author{\textbf{Abdalraheem A. Ijjeh }} 
% logo align to Institute 
\institute{Postdoc at GRVC Robotics Laboratory \\
	 University of Seville, Spain
	 \vfill
	 \vfill
     \includegraphics[width=2cm]{logo_grvc-removebg-preview.png}
     \vspace{25pt}
     \includegraphics[width=6cm]{imp_logo.png}	
  }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\tikzexternalize % activate!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setbeamertemplate{section in toc}[sections numbered]
\setbeamertemplate{subsection in toc}[subsections numbered]

\begin{document}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\maketitle
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\note{
		Hello everyone, and welcome.
		My name is Abdalraheem Ijjeh.
		This course will be an introduction to the artificial intelligence approaches utilised in structural health monitoring and non destructive testing. 
		I have recently completed my PhD at the IFFM, PAS, and my supervisor during my PhD was Professor Pawel Kudela.
		Currently, I am a post doc at the GRVC robotics lab in Spain.
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	% SLIDES
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{frame}[label=frame1]{Outlines}
		\begin{multicols}{2}
			%		\fontsize{6pt}{8pt}\selectfont
			\setbeamertemplate{section in toc}[sections numbered]
			\setbeamertemplate{subsection in toc}[subsections numbered]
			\tableofcontents
		\end{multicols}
	\end{frame}	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\note{The course  will be as follows: 
		In the first section, I will briefly talk about Structural health monitoring and non destructive testing.	
		Then, in the second section we will talk about artificial intelligence, machine learning and deep learning technologies.
		In section three, we will see how to utilise AI approaches for anomaly detection.
		And then, we will go little bit into basics of deep learning.
		In section Four, I will present a case study of utilising artificial intelligence for delamination identification in composite materials.
		Hence, we will talk briefly about guided waves (Lamb waves).
		And how the synthetic dataset was generated in order to train DL models 
		Next, I will present the developed deep-learning models for damage identification and their evaluation. 
		Finally, the conclusions will be presented in section 5.
	}
%	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	\section{Motivation}
%	\begin{frame}{Defects in composite laminates}
%		\small
%		Composite laminates can have different types of damage such as: \\
%		\textbf{Cracks, fibre breakage, debonding, and \alert{delamination}}.
%		\begin{columns}[T]
%			\begin{column}[c]{.45\textwidth}
%				\begin{itemize}
%					\footnotesize
%					\item Delamination is a critical failure mechanism in laminated fibre-reinforced polymer matrix composites.
%					\item Delamination is one of the most hazardous forms of the defects. 
%					It leads to very catastrophic failures if not detected at early stages.
%				\end{itemize}
%			\end{column}
%			\begin{column}[c]{0.50\textwidth}
%				\begin{figure}
%					\includegraphics[width=.95\textwidth]{delaminated_plate1.jpg}
%				\end{figure}
%			\end{column}
%		\end{columns}
%	\end{frame}
%	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	\note{
%		Composite laminates have a wide range of applications in various industries due to their characteristics, such as high strength, low density, and resistance to fatigue and corrosion, among others.
%		
%		However, defects such as cracks, fibre breakage, and debonding can occur in composite laminates. 
%		
%		In particular, laminated composite materials are more sensitive to damage in the form of delamination due to weak transverse tensile and interlaminar shear strengths.
%			
%		Delaminations can seriously decrease the performance of composite structures.
%		Accordingly, delamination detection in its early stages can significantly help to avoid catastrophic structural collapses.		
%	}
%	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	\section{Objectives}
%	\begin{frame}{Objectives}
%		\textbf{To develop \textcolor{blue}{a novel AI-driven diagnostic system} for delamination identification in composite laminates such as carbon fibre reinforced polymers (CFRP).}
%		\vfil
%		\textbf{To address the issue of \textcolor{blue}{slow data acquisition} by SLDV of high-resolution full wavefields of Lamb wave propagation.}
%		\begin{alertblock}{Thesis}
%			It is possible to use an end-to-end approach in which DNN 
%			processes the animation of propagating waves (input) directly into a damage map (output).
%		\end{alertblock}
%	\end{frame}
%	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	\note{
%		The main objective of this work is to develop an artificial intelligence-based diagnostic system for the detection of delaminations in composite laminates.
%		
%		Therefore, I investigated the possibility of embracing the end-to-end approach by using animations of Lamb wave propagation in a plate interacting with discontinuities such as damage and edges with an artificial intelligence-based approach to detect and identify delaminations.
%		
%		My second objective is to address the issue of slow data acquisition of high-resolution full wavefields of Lamb wave propagation by SLDV. 		
%	}
%	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{Introduction to SHM/NDT}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{frame}{Structural Health Monitoring (SHM)}
		\begin{columns}[T]
			\begin{column}[c]{0.3\textwidth}
				\justifying
				Structural Health Monitoring (SHM) refers to the process of continuously monitoring and evaluating the condition of structures to ensure their safety and performance.
			\end{column}
			\begin{column}[c]{.6\textwidth}
				\begin{figure}
					\includegraphics[height=.6\textwidth]{SHM_system.png}
				\end{figure}
			\end{column}
		\end{columns}
	\end{frame}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\note{
		Structural health monitoring is about monitoring a structure periodically or continuously to evaluate its technical conditions.
		
		As a result, the SHM aims to place integrated sensors on the structure to continuously monitor its condition.
		
		In this way, structural health monitoring mimics the human nervous system, which can provide us with real-time readings and detect any abnormal events.
		
		Accordingly, this is the ultimate aim of SHM: to monitor in real-time in a way that can detect damage early so we can perform preventive measures to protect the structure.
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{frame}{Non-destructive testing (NDT)}
		\begin{columns}[T]
			\begin{column}[c]{.35\textwidth}
				\justifying
				NDT techniques are used to inspect materials, components, and structures without causing permanent damage or altering their integrity. 
			\end{column}
			\begin{column}[c]{.65\textwidth}
				\includegraphics[width=0.95\textwidth]{NDT.png}
			\end{column}
		\end{columns}
	\end{frame}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\note{
		NDT techniques are used to inspect materials, components, and structures without causing permanent damage or altering their integrity. 
		These techniques are employed across various industries to ensure the quality, reliability, and safety of products and infrastructure. Some common non-destructive testing techniques include:
		
		Visual Inspection: The simplest and most widely used technique, involving direct visual examination to identify surface defects, cracks, corrosion, or other abnormalities.
		
		Ultrasonic Testing (UT): Utilizes high-frequency sound waves to detect internal flaws or changes in material properties. Ultrasonic waves are directed into the test object, and the reflections or changes in the waves are analysed to identify defects.
		
		Radiographic Testing (RT): Involves the use of X-rays, gamma rays, or other penetrating radiation to inspect internal structures or welds. The radiation passes through the object, and the resulting image reveals any defects or inconsistencies.
		
		Magnetic Particle Testing (MT): Detects surface or near-surface defects in ferromagnetic materials. The material is magnetized, and magnetic particles are applied to the surface. The particles accumulate at defect locations, making them visible for inspection.
		
		Liquid Penetrant Testing (PT): A surface inspection method where a liquid dye or fluorescent penetrant is applied to the material. The penetrant seeps into surface cracks or flaws, and excess penetrant is removed. A developer is then applied, highlighting any defects through dye bleeding or fluorescence.
		
		Eddy Current Testing (ECT): Primarily used for conductive materials, this technique utilizes electromagnetic induction to detect flaws or changes in electrical conductivity. Eddy currents are induced in the material, and variations in the electrical response are analysed for defect identification.
		
		Acoustic Emission Testing (AET): Monitors acoustic signals emitted by a material during stress or deformation. It is useful for detecting active defects or ongoing damage in structures under load.
		
		Infrared Thermography (IRT): Utilizes thermal imaging cameras to detect and analyse thermal patterns on the surface of an object. It helps identify anomalies, such as heat loss, hotspots, or defects, by detecting variations in temperature.
		
				
		These techniques, among others, are chosen based on the material, the type of defect or inspection required, and the specific industry standards and regulations in place. 
		Each method has its strengths and limitations, and NDT professionals select the appropriate technique or combination of techniques to ensure accurate and reliable inspection results.
		
	}
	\setcounter{subfigure}{0}
	\section{Artificial intelligence, machine learning, and deep learning}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{frame}{What is AI?}
		\begin{figure}
				\centering
				\includegraphics[width=0.85\textwidth]{AI_vs_ML_vs_Deep_Learning.png}
			\end{figure}
		\tiny
		(source: https://www.ingeniovirtual.com/)
	\end{frame}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\note{
		Artificial Intelligence (AI), Machine Learning (ML), and Deep Learning (DL) are related concepts within the field of computer science. Here's a brief explanation of each:
		
		Artificial Intelligence (AI): AI refers to the development of computer systems that can perform tasks that typically require human intelligence. It encompasses a wide range of techniques and methodologies that enable machines to perceive, reason, learn, and make decisions. AI systems aim to simulate human intelligence and exhibit behaviours like problem-solving, speech recognition, image understanding, and decision-making.
		
		Machine Learning (ML): Machine Learning is a subset of AI that focuses on enabling machines to learn and improve from experience without being explicitly programmed. Instead of relying on explicit instructions, ML algorithms learn patterns and insights from data and make predictions or take actions based on that learning. ML algorithms can automatically identify and learn from patterns, relationships, and trends within large datasets, enabling them to make accurate predictions or perform specific tasks.
		
		Deep Learning (DL): Deep Learning is a subfield of Machine Learning that is inspired by the structure and function of the human brain's neural networks. It involves the use of artificial neural networks with multiple layers (deep neural networks) to learn and extract complex representations of data. DL algorithms are particularly effective at processing and understanding large amounts of unstructured data, such as images, videos, text, and audio. Deep Learning has achieved remarkable success in tasks like image and speech recognition, natural language processing, and more.
		
		In summary, Artificial Intelligence is a broad field that encompasses the development of intelligent systems, while Machine Learning is a subset of AI that focuses on enabling machines to learn from data. Deep Learning, in turn, is a specialized technique within Machine Learning that leverages deep neural networks to process and understand complex data representations.
	}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\setcounter{subfigure}{0}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{frame}{Why now?}
		\begin{columns}[T]
			\begin{column}[c]{0.4\textwidth}
				AI technologies are in accelerating growth due to:
				\begin{itemize}
					\item Exponential development in computer hardware industries
					(e.g. CPUs, GPUs, FPGAs, TPUs and ASICs)
					\item Era of Big data.
				\end{itemize}
			\end{column}
			\begin{column}[c]{0.55\textwidth}
				\begin{figure}
					\centering
					\subfloat{\animategraphics[autoplay,loop,width=.9\textwidth]{10}{gif_figs/gpu/gpu_-}{0}{34}}
				\end{figure}
				\tiny
				(source: https://www.techbooky.com/)
			\end{column}
		\end{columns}		
	\end{frame}
	\note{
		AI and deep learning have gained significant attention and popularity in recent years due to several key factors:
		
		Increased Computing Power: The advancement of computer hardware, particularly in terms of processing power and parallel computing capabilities, has enabled the efficient training and deployment of complex AI models. Deep learning algorithms, which are computationally intensive, have greatly benefited from these advancements, allowing for faster and more accurate model training.
		
		Big Data Availability: The proliferation of digital technologies and the internet has generated massive amounts of data across various domains. Deep learning models excel at leveraging large datasets to extract meaningful patterns and insights. With the availability of big data, AI and deep learning have been able to demonstrate their potential in solving complex problems and making accurate predictions.
		
		Breakthroughs in Research: Researchers and practitioners have made significant advancements in AI and deep learning techniques, introducing novel algorithms and architectures that have achieved impressive results across various domains. Notable breakthroughs, such as the development of convolutional neural networks (CNNs) for image recognition and recurrent neural networks (RNNs) for natural language processing, have fueled the growth and adoption of deep learning.
		
		Practical Applications and Industry Adoption: AI and deep learning have demonstrated their effectiveness and potential in solving real-world problems across multiple industries. Applications include image and speech recognition, natural language processing, recommendation systems, autonomous vehicles, healthcare diagnostics, and more. These practical applications have garnered significant interest and investment from businesses, driving the demand for AI and deep learning technologies.
		
		Open-Source Frameworks and Tools: The availability of open-source deep learning frameworks like TensorFlow, PyTorch, and Keras has democratized access to AI and deep learning capabilities. These frameworks provide developers and researchers with powerful tools to build and experiment with complex models, fostering innovation and collaboration within the AI community.
		
		
		Collectively, these factors have contributed to the current "hot" status of AI and deep learning, with widespread interest, research advancements, industry adoption, and transformative applications across diverse fields.
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\setcounter{subfigure}{0}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{frame}{Common learning strategies}
		\centering
		\begin{figure}
			\includegraphics[width=0.9\textwidth]{learning.png}
		\end{figure}
	\end{frame}
	\note{
	In artificial intelligence, there are several common learning strategies or approaches used to train models and enable them to acquire knowledge from data. Here are some of the key learning strategies:
	
	Supervised Learning: In supervised learning, a model is trained on a labelled dataset where both input features and corresponding output labels are provided. The model learns to map inputs to outputs by minimizing the discrepancy between its predictions and the true labels. This approach is commonly used for tasks like classification and regression.
	
	Unsupervised Learning: Unsupervised learning involves training a model on an unlabelled dataset. The goal is to discover hidden patterns, structures, or relationships in the data without any predefined output labels. Clustering, dimensionality reduction, and anomaly detection are examples of unsupervised learning tasks.
	
	Semi-Supervised Learning: Semi-supervised learning combines labelled and unlabelled data to train a model. It leverages the small amount of labeled data along with the larger pool of unlabelled data to improve performance. This approach is useful when obtaining labelled data is expensive or time-consuming.
		
	Reinforcement Learning: Reinforcement learning involves training an agent to interact with an environment and learn optimal actions based on feedback in the form of rewards or punishments. The agent learns through trial and error to maximize cumulative rewards, discovering a policy that guides its decision-making. Reinforcement learning is often used in scenarios such as game playing, robotics, and control systems.
	
	Transfer Learning: Transfer learning involves training a model on one task and then applying the learned knowledge to a different but related task. The idea is to transfer the learned representations or knowledge from the source task to the target task, which can help in cases where the target task has limited labelled data available.
	
	Generative Modeling: Generative models aim to learn the underlying probability distribution of the data in order to generate new samples that are similar to the training data. Examples include generative adversarial networks (GANs) and variational autoencoders (VAEs). Generative models have applications in image synthesis, text generation, and data augmentation.
	
	These are some of the common learning strategies in artificial intelligence. Different approaches are suitable for different problem domains, and often a combination of these strategies is used to tackle complex tasks or real-world scenarios.
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\section{AI approaches for Anomaly detection}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{frame}{Machine learning approach}
		Machine learning techniques involve two processes:
		\alert{\textbf{Feature extraction and classification}}
		\begin{itemize}
			\item \alert{\textbf{Feature extraction}} \(\rightarrow\) includes signals preprocessing (signal denoising and averaging), then extracting \textbf{damage indexes (DIs)}.
			\item \alert{\textbf{Feature classification}}\(\rightarrow\) the extracted features are classified, for instance, into healthy or damaged states.
		\end{itemize}
		\begin{figure}
			\centering
			\includegraphics[width=.95\textwidth]{conventional_ML.png}
		\end{figure}
	\end{frame}
	\note{
		Feature extraction is a crucial step in machine learning and pattern recognition tasks. It involves transforming raw data into a representative set of features that capture relevant information for the given problem. 
		Here are some commonly used techniques for feature extraction:
		
		Statistical Features: Statistical measures such as mean, median, standard deviation, skewness, and kurtosis can capture important characteristics of the data distribution.
		
		Fourier Transform: The Fourier transform decomposes a signal into its frequency components, revealing the presence of periodic patterns or oscillations in the data. Frequency domain features like power spectral density or dominant frequencies can be extracted.
		
		Wavelet Transform: The wavelet transform represents signals at multiple scales, capturing both time and frequency information. Wavelet-based features are useful for analyzing transient or non-stationary signals.
		
		Principal Component Analysis (PCA): PCA is a dimensionality reduction technique that identifies the most important orthogonal directions (principal components) in the data. It helps to capture the maximum variance and reduce the dimensionality of the feature space.
		
		Independent Component Analysis (ICA): ICA aims to separate a multivariate signal into additive subcomponents, assuming they are statistically independent. ICA is useful for separating mixed signals or sources from a set of observations.
		
		Mel-Frequency Cepstral Coefficients (MFCC): MFCC is commonly used for audio signal processing. It involves transforming the audio spectrum using the mel scale and extracting cepstral coefficients that represent the spectral envelope of the signal.
		
		Image-based Features: In computer vision, features such as edge detection, texture descriptors (e.g., Local Binary Patterns), color histograms are commonly used to represent images.
	
		The choice of feature extraction technique depends on the nature of the data, the problem at hand, and the available domain knowledge. It often requires experimentation and domain expertise to identify the most informative set of features for a given task.
		
		Feature classification, also known as feature learning or pattern recognition, is the process of assigning or labeling data samples into predefined classes or categories based on the extracted features. It is a fundamental task in machine learning and is often used for tasks such as object recognition, image classification, text categorization, and speech recognition. Various techniques can be employed for feature classification, including:
		
		Supervised Learning: In supervised learning, labeled training data is used to train a classification model that can map input features to their corresponding class labels. Some commonly used supervised learning algorithms include:
		
		Decision Trees
		Random Forests
		Support Vector Machines (SVM)
		Naive Bayes
		k-Nearest Neighbors (k-NN)
		Logistic Regression
		Neural Networks (e.g., Multi-layer Perceptron)
		Unsupervised Learning: Unsupervised learning does not rely on labeled data and aims to identify patterns or groupings in the data without explicit class labels. Clustering algorithms are commonly used for unsupervised feature classification:
		
		K-means Clustering
		Hierarchical Clustering
		DBSCAN (Density-Based Spatial Clustering of Applications with Noise)
		Gaussian Mixture Models (GMM)
		
		The choice of classification technique depends on various factors, including the nature of the data, the number of classes, the availability of labeled data, the desired computational efficiency, and the desired interpretability of the model. It often requires experimentation and tuning to determine the most suitable technique for a given classification task.
	}
	\begin{frame}{Drawbacks of Conventional methods}
		Conventional machine learning methods have several drawbacks, including:
		\begin{itemize}
			\item Feature Engineering
			\item Limited Ability to Handle High-Dimensional Data
			\item Lack of Scalability
			\item Lack of Adaptability to Changing Data
			\item Limited Representation Learning
%			\item Interpretability	
		\end{itemize}
	\end{frame}

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\note{
		Conventional machine learning methods have several drawbacks, including:
		
		Feature Engineering: Conventional machine learning often requires manual feature engineering, where domain experts have to identify and select relevant features from the raw data. This process can be time-consuming, subjective, and may not always capture the full complexity of the data.
		
		Limited Ability to Handle High-Dimensional Data: Conventional methods can struggle when faced with high-dimensional data, such as images or text. The curse of dimensionality can lead to increased computational complexity and overfitting.
		
		Lack of Scalability: Some conventional machine learning algorithms, such as Support Vector Machines (SVMs), have computational complexity that scales poorly with large datasets. Training these models on massive datasets can become prohibitively time-consuming and resource-intensive.
		
		Manual Hyperparameter Tuning: Many conventional algorithms require careful tuning of hyperparameters, such as learning rates or regularization parameters, to achieve optimal performance. This process often relies on trial and error or expert knowledge, which can be labor-intensive and may not guarantee the best results.
		
		Lack of Adaptability to Changing Data: Conventional models are often trained on fixed datasets and may not easily adapt to changes in the underlying data distribution. This can make them less suitable for applications where the data distribution evolves over time, such as in online learning or real-time data streams.
		
		Limited Representation Learning: Traditional machine learning algorithms often struggle to automatically learn meaningful representations from raw data. They heavily rely on handcrafted features, which can be suboptimal or fail to capture the intricacies of complex patterns in the data.
		
		
		It's important to note that while conventional machine learning methods have their limitations, they have been successfully applied in numerous domains and continue to be valuable in many practical applications. However, newer approaches, such as deep learning and other advancements in artificial intelligence, have emerged to address some of these drawbacks and push the boundaries of what is achievable with machine learning.
	
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\setcounter{subfigure}{0}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%	
	\begin{frame}{Deep learning approach}
		\textbf{Deep learning (DL) technologies are in accelerating growth due to:}
		\begin{itemize}
			\item \textcolor{blue}{Exponential development in computer hardware/software industries.}
			\item \textcolor{blue}{Machine learning algorithms.}
			\item \textcolor{blue}{Era of Big data.}
		\end{itemize}	
		Deep learning offers an \alert{\textbf{end-to-end}} approach: \alert{\textbf{Automatic}} feature extraction and classification.
		\begin{figure}
			\includegraphics[width=.95\textwidth]{DL_approach.png}
		\end{figure}
	\end{frame}
	\note{The end-to-end approach in deep learning refers to a modeling paradigm where a single, unified neural network is trained to directly map raw input data to desired output predictions or actions, without relying on explicit intermediate representations or hand-engineered features.
		
		Traditionally, in many machine learning pipelines, multiple stages of processing are involved, such as data preprocessing, feature extraction, and model building. These stages often require domain-specific knowledge and expert feature engineering. In contrast, the end-to-end approach aims to automate this process by allowing the deep neural network to learn representations and features directly from the raw input data.
		
		Here are some key characteristics of the end-to-end approach:
		
		Raw Input to Output Mapping: The end-to-end approach involves training a deep neural network to learn a direct mapping from raw input data (such as images, audio, text) to desired output predictions or actions (such as object recognition, speech transcription, language translation). The network takes raw data as input and produces the desired output without explicit feature engineering or preprocessing steps.
		
		Automatic Feature Learning: With the end-to-end approach, the deep neural network learns to extract relevant features and representations from the raw input data during the training process. This eliminates the need for manual feature engineering, as the network automatically discovers and learns useful representations that are relevant for the given task.
		
		Complex Mapping and Hierarchical Representations: Deep neural networks excel at learning complex and hierarchical representations from data. By utilizing multiple layers of interconnected nodes (neurons), these networks can capture and learn intricate patterns and relationships in the data. The end-to-end approach leverages the power of deep learning models to handle high-dimensional and complex input data and learn the appropriate representations for the given task.
		
		Simplicity and Integration: The end-to-end approach simplifies the overall modeling pipeline by removing the need for separate stages of feature extraction and preprocessing. It allows for a more seamless integration of data processing and model training, potentially reducing the complexity of the overall system and making it easier to develop end-to-end solutions.
		
		While the end-to-end approach offers benefits such as reduced reliance on domain expertise and streamlined development, it may also pose challenges in cases where fine-grained control, or understanding of intermediate steps is necessary. 
		Nevertheless, in many domains, the end-to-end approach has shown promising results and has been successfully applied to a wide range of tasks, including computer vision, natural language processing, and speech recognition.}
	
	\subsection{A dive into DL}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{frame}{Artificial neural networks (ANNs)}
		\begin{columns}[T]
			\begin{column}{.45\textwidth}
				\begin{block}{Artificial neuron}
					\begin{figure}
						\centering
						\includegraphics[width=.95\textwidth]{ArtificialNeuronModel.png}
					\end{figure}
				The equation for an artificial neuron is given by:
				\[
				y = \sigma\left(\sum_{i=1}^{n} w_i x_i + b\right)
				\]
				\end{block}
			\end{column}
			\begin{column}{0.55\textwidth}
				\begin{block}{ANN}
					\begin{figure}
						\centering
						\includegraphics[width=0.85\textwidth]{ANN.png}
					\end{figure}
					ANN can be created by cascading n-layers of “neurons”. 
					\begin{itemize}
						\item	Input layer.
						\item	Hidden layer(s).
						\item	Output layer.	
					\end{itemize}
				\end{block}
			\end{column}
		\end{columns}
	\end{frame}
	\note{
	
	An Artificial Neural Network (ANN) is a computational model inspired by the structure and function of biological neural networks, such as the human brain. 
	It consists of interconnected artificial neurons, also called nodes or units, organized in layers. 
	Each node receives inputs, performs computations, and produces an output that is passed to other nodes.
	
	The equation for a simple artificial neuron, often referred to as a perceptron, can be represented as follows:
	
	Here's a breakdown of the components in the equation:
	
	Weighted Sum: The inputs to the neuron are multiplied by corresponding weights. 
	These weights represent the strength or importance of each input. The weighted sum is calculated by adding the products of the inputs and their respective weights.
	
	Bias: A bias term is added to the weighted sum. 
	The bias provides the flexibility to shift the activation function's output and helps the neural network account for situations where all inputs are zero or when a non-zero output is desired.
	
	Activation Function: The weighted sum plus bias is passed through an activation function, which introduces non-linearity into the output of the neuron. 
	The activation function determines whether the neuron is "activated" or not based on the calculated value. 
	It adds complexity and non-linear behavior to the neural network, enabling it to model more complex relationships in the data.
	
	Output: The output of the neuron is the result of the activation function applied to the weighted sum plus bias. It is typically the input to other neurons in the network or the final output of the network itself.
	
	This equation is the basic building block of an artificial neural network, and it is applied to each neuron in the network's layers. 
	By adjusting the weights and biases during the training process, the network can learn to make accurate predictions or perform specific tasks based on the given inputs.
	
	Note that the equation described above represents a simplified version of an artificial neuron. 
	In practice, there are various types of activation functions, network architectures, and learning algorithms used in neural networks, such as multilayer perceptrons (MLPs), convolutional neural networks (CNNs), recurrent neural networks (RNNs), and more, each with their own equations and variations.	
}
	\setcounter{subfigure}{0}
	\begin{frame}{Optimization and Deep Learning}
		\begin{columns}[T]
			\begin{column}[c]{.5\textwidth}
				\begin{itemize}
					\item \alert{Labelled data} 
					\item \alert{Loss function}
					\item \alert{Learnable parameters}
				\end{itemize}
				\begin{block}{Backpropagation}
					\begin{figure}
						\centering
						\animategraphics[autoplay,loop,width =1.0\textwidth]{1}{figures/gif_figs/BP/png/BP_technique_}{0}{14}
					\end{figure}
				\end{block}
			\end{column}
			\begin{column}[t]{.5\textwidth}
				\centering
				\includegraphics[height=0.40\textheight]{Gradient_decent.png}
				\\
				\includegraphics[height=0.40\textheight]{local_minima.png}
			\end{column}
		\end{columns}		
	\end{frame}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\note{			
		The optimization process in deep learning refers to the training of a neural network model to find the optimal set of weights and biases that minimize the discrepancy between the predicted outputs and the desired outputs for a given training dataset. 
		This process involves iteratively adjusting the model's parameters using an optimization algorithm.
		
		Here's an overview of the optimization process in deep learning:
		
		Loss Function: The first step is to define a loss function, also known as an objective or cost function. The loss function measures the discrepancy between the predicted outputs of the model and the true values in the training data. The choice of the loss function depends on the specific task at hand, such as mean squared error (MSE) for regression problems or cross-entropy loss for classification problems.
		
		Initialization: The model's weights and biases are initialized with random values. Proper initialization is important to avoid the model getting stuck in poor local optima that  is a point in the parameter space where the objective function reaches the lowest value within a local neighborhood. 
		It means that compared to other nearby points, the local optimum has a lower value of the objective function. However, it does not guarantee that it is the global minimum, which is the absolute lowest point in the entire parameter space..
		
		Forward Propagation: During the forward propagation step, input data is fed into the model, and it performs computations and generates predictions using the current set of parameters. The predicted outputs are compared with the true outputs using the chosen loss function.
		
		Backward Propagation (Backpropagation): Backpropagation is a key component of training deep neural networks. It involves calculating the gradients of the loss function with respect to the model's parameters (weights and biases). This process starts from the output layer and moves backward through the layers, using the chain rule of calculus to efficiently compute the gradients layer by layer.
		
		Parameter Update: With the gradients computed, an optimization algorithm is used to update the model's parameters. The most commonly used optimization algorithm is Stochastic Gradient Descent (SGD) and its variants. These algorithms adjust the weights and biases in a way that moves the model parameters in the direction that reduces the loss function.
		
		Iterations: Steps 3-5 (forward propagation, backward propagation, and parameter update) are repeated iteratively for a certain number of epochs or until convergence. Each iteration helps to refine the model's parameters and reduce the loss. The training process continues until the model reaches a satisfactory level of accuracy or until a stopping criterion is met.
		
		Validation and Testing: Once the training is complete, the model's performance is evaluated using a separate validation set or held-out test set. This evaluation provides insights into the model's generalization and helps to assess its performance on unseen data.
		
		The optimization process in deep learning is computationally intensive and often requires powerful hardware and efficient implementations. Various techniques and improvements have been developed to enhance optimization, such as learning rate schedules, regularization techniques (e.g., dropout, weight decay), and advanced optimization algorithms (e.g., Adam, RMSprop).
		
		By iteratively adjusting the model's parameters through the optimization process, deep learning models can effectively learn complex patterns and representations in the data, leading to improved performance on various tasks.
	}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{frame}{Convolution Neural Networks (CNNs)}
		\noindent CNN is one of the most utilised architectures in DL for  \alert{recognising complex patterns} by performing .
		
		Convolution in DL is a cross-correlation operation \alert{(sliding dot product)}. \\
		The kernel weights \alert{(learnable weights)} are updated during training phase.
		\begin{figure}[t]
			\centering
			\animategraphics[autoplay,loop,width =0.95\textwidth]{1}{figures/gif_figs/files/plot_convolution_process_}{0}{32}
		\end{figure}
	\end{frame}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\note{
		Convolutional Neural Network (CNN) is a powerful deep learning architecture for image processing because it can extract complex feature patterns from images using convolution operations.
		
		The convolution operation for image processing is essentially a cross-correlation operation, also known as a sliding dot product or sliding inner product.
		
		The kernel slides over an input image of performing a convolution operation.
		The output of the convolution operation is a feature map.
		
		Consequently, kernels learn to detect different types of edges
		(vertical, horizontal, and diagonal edges), colour intensities, etc.	
		
		During the backpropagation process, all kernel weights are updated. 				
	}
	\begin{frame}{CNN architecture}
		\begin{columns}[T]
			\begin{column}[c]{0.7\textwidth}
				\begin{block}{CNN}
					\begin{figure}
						\centering
						\includegraphics[width=0.9\textwidth]{cnn_ann.png}
					\end{figure}
				\end{block}
			\end{column}
		\begin{column}[c]{0.3\textwidth}
			\begin{block}{Downsampling}
					\begin{figure}
						\includegraphics[width=0.5\textwidth]{downsampling.png}
					\end{figure}
				\end{block}
			\end{column}
		\end{columns}
	\end{frame}
	\note{
		The general architecture of a Convolutional Neural Network (CNN) consists of multiple layers designed to effectively process and extract features from input data, such as images. Here is an overview of the typical layers and their role in a CNN:
		
		Input Layer: The input layer receives the raw input data, which is usually an image or a collection of images. The input data is typically represented as a 3D tensor with dimensions (height, width, channels), where channels refer to the color channels (e.g., red, green, blue) of an image.
		
		Convolutional Layers: Convolutional layers are the key components of a CNN. They consist of filters or kernels that convolve over the input data to perform feature extraction. Each filter performs a set of mathematical operations (convolution) on a small receptive field of the input, producing a feature map that highlights specific patterns or features present in the input. Multiple filters are used to extract different features. The convolutional layers are responsible for learning hierarchical representations of the input data.
		
		Pooling Layers: Pooling layers (e.g., Max Pooling, Average Pooling) reduce the spatial dimensions of the feature maps obtained from the convolutional layers. They achieve this by downsampling and summarizing the information within local regions of the feature maps. Pooling helps to extract the most salient features while reducing the computational burden and making the model more robust to variations in the input.
		
		Fully Connected Layers: After several convolutional and pooling layers, the feature maps are flattened into a 1D vector and passed to fully connected layers. These layers connect every neuron in one layer to every neuron in the subsequent layer. Fully connected layers learn high-level representations by combining the low-level features extracted by the earlier layers. The output of the final fully connected layer is typically fed into a softmax or sigmoid activation function to produce the network's predictions or class probabilities.
		
		Output Layer: The output layer produces the final predictions or outputs of the CNN. The number of neurons in the output layer depends on the specific task. For example, in image classification, the output layer might have neurons corresponding to the number of classes to predict.
		
		This general architecture, often referred to as a "vanilla" CNN, can be customized and extended in various ways to suit different tasks and improve performance. Variants of CNNs, such as architectures with skip connections (e.g., ResNet), inception modules (e.g., GoogLeNet), or recurrent connections (e.g., ConvLSTM), have been developed to address specific challenges and achieve state-of-the-art results in tasks like image classification, object detection, and semantic segmentation.}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\setcounter{subfigure}{0}
	\begin{frame}{Recurrent Neural Networks (RNNs)}
		\begin{columns}[T]
			\begin{column}[c]{0.4\textwidth}
				\justifying
				\begin{itemize}
					\item RNNs are a type of neural network specifically designed to handle sequential data.
					\item They have connections between the hidden units that form a directed cycle, allowing information to persist.
					\item RNNs are widely used in natural language processing, speech recognition, time series analysis, etc.
				\end{itemize}
			\end{column}
			\begin{column}[c]{0.55\textwidth}
				\begin{block}{RNN}
					\begin{figure}
						\includegraphics[width=0.95\textwidth]{unrolled_rnn.png}
					\end{figure}
				\end{block}				
			\end{column}
		\end{columns}
	\end{frame}
	\note{
		A Recurrent Neural Network (RNN) is a type of neural network designed to process sequential data by maintaining and utilizing an internal memory or hidden state. Unlike feedforward neural networks, which process each input independently, RNNs can capture information from previous inputs and use it to influence the current prediction or output.
		
		The key feature of RNNs is their ability to handle input sequences of varying lengths. They achieve this by maintaining a hidden state that gets updated at each time step and serves as a memory of the past inputs. The hidden state is passed along through time, allowing the network to capture dependencies and temporal information in the data.
		
		Here are the main components and characteristics of an RNN:
		
		Hidden State: The hidden state is a vector that captures the network's memory of past inputs. It is updated at each time step based on the current input and the previous hidden state. The hidden state acts as a representation of the sequence history and is shared across all time steps, allowing the RNN to process sequential information.
		
		Recurrent Connections: RNNs have recurrent connections that enable information to flow from one time step to the next. These connections allow the hidden state to carry information about past inputs and influence future predictions. The recurrent connections make RNNs well-suited for tasks that require sequential processing, such as natural language processing, speech recognition, and time series analysis.
		
		Time Steps: RNNs operate over a series of time steps, with each time step corresponding to an element in the input sequence. At each time step, the RNN takes an input and updates its hidden state based on the current input and the previous hidden state. This iterative process continues until the entire sequence has been processed.
		
		Training and Backpropagation Through Time (BPTT): RNNs are trained using a variant of backpropagation called Backpropagation Through Time (BPTT). BPTT unfolds the recurrent connections of the network through time, allowing the gradients to flow back from the output to the earlier time steps. This enables the network to learn from the entire sequence and adjust its parameters to minimize the error between predicted and target outputs.
		
		Types of RNNs: There are different types of RNN architectures, including the basic RNN, Long Short-Term Memory (LSTM), and Gated Recurrent Unit (GRU). LSTM and GRU are popular variations of RNNs that address the vanishing gradient problem and allow for more effective handling of long-term dependencies.
		
		RNNs have demonstrated success in tasks involving sequential data, such as language modeling, machine translation, speech recognition, sentiment analysis, and time series forecasting. However, they can face challenges in capturing long-term dependencies or handling very long sequences due to limitations like vanishing or exploding gradients. More advanced architectures, such as attention mechanisms and Transformer models, have been developed to overcome these limitations and achieve improved performance on sequence-related tasks.
	}
	\setcounter{subfigure}{0}
	\begin{frame}{Long Short Term Memory (LSTM)}
		\begin{columns}[T]
			\begin{column}[c]{0.45\textwidth}
				\justifying
				\begin{itemize}
					\item LSTM is an advanced variant of RNNs that addresses the vanishing gradient problem.
					\item It introduces memory cells, input gates, forget gates, and output gates to regulate information flow.
					\item LSTM can selectively retain or discard information, allowing long-term dependencies to be learned effectively.
				\end{itemize}
			\end{column}
			\begin{column}[c]{0.55\textwidth}
				\begin{block}{LSTM}
					\begin{figure}
						\includegraphics[width=0.65\textwidth]{lstm.png}
					\end{figure}
				\begin{columns}
					\begin{column}[c]{0.45\textwidth}
						\begin{itemize}
							\item \alert{Memory Cell}
							\item \alert{Input Gate}
						\end{itemize}
					\end{column}
					\begin{column}[c]{0.45\textwidth}
						\begin{itemize}					
							\item \alert{Forget Gate}
							\item \alert{Output Gate}
						\end{itemize}
					\end{column}
				\end{columns}					
				\end{block}				
			\end{column}
		\end{columns}
	\end{frame}
	\note{
		Advantages of LSTM:
		\begin{itemize}
			\item LSTM can capture long-term dependencies in sequences.
			\item It has been successfully applied to various tasks like language modelling, speech recognition, machine translation, etc.
			\item LSTM is capable of processing inputs of varying length.
			\item It can learn and remember relevant information while ignoring irrelevant information.			
		\end{itemize}
	\begin{itemize}
		\item \alert{Memory Cell}: Stores information over time.
		\item \alert{Input Gate}: Controls the flow of new information into the memory cell.
		\item \alert{Forget Gate}: Controls the flow of old information out of the memory cell.
		\item \alert{Output Gate}: Controls the flow of information from the memory cell to the output.
	\end{itemize}
	}
	\setcounter{subfigure}{0}
	\begin{frame}{Encoder-decoder (Autoencoder)}
		\begin{columns}[T]
			\begin{column}[t]{.35\textwidth}
				\begin{itemize}
					\item \alert{Encoder} learns to extract features.
					\item \alert{Latent space} has condensed feature maps.
					\item \alert{Decoder} learns to locate the features learned by the encoder.
				\end{itemize}	
			\end{column}
			\hfill
			\begin{column}[t]{.6\textwidth}
				\begin{figure}
					\centering
					\includegraphics[width=1\textwidth]{nn_encoder_decoder.png}
				\end{figure}	
			\end{column}
		\end{columns}			
	\end{frame}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\note{
		The encoder-decoder is a well-known architecture used for tasks such as computer vision.
		The encoder aims to produce compressed feature maps from the input image at various scale levels using cascaded convolutions and downsampling operations. 
		The decoder is responsible for upsampling the condensed feature maps in the latent space to the original input shape.		
	}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{frame}{Things to keep in mind!}
		When developing a supervised deep learning model, there are several important considerations to keep in mind:
		\begin{itemize}
			\item Sufficient and Representative Training Data
			\item Data Preprocessing and Augmentation
			\item Model Architecture
			\item Hyperparameter Tuning
			\item Regularization and Overfitting
			\item Choice of Loss Function
			\item Training Strategy
			\item Model Evaluation
			\item Hardware and Computational Resources	
		\end{itemize}
	\end{frame}
	\note{When developing a supervised deep learning model, there are several important considerations to keep in mind:
		
		Sufficient and Representative Training Data: Deep learning models typically require large amounts of labeled training data to generalize well. Ensure that your training dataset is representative of the target population and contains enough diverse examples to capture the underlying patterns effectively.
		
		Data Preprocessing and Augmentation: Preprocess your data appropriately by handling missing values, normalizing features, and addressing class imbalance if present. Consider applying data augmentation techniques, such as rotation, scaling, or flipping, to artificially increase the diversity of your training data.
		
		Model Architecture: Design an appropriate architecture for your deep learning model. Consider the complexity and depth of the model, the type of layers (e.g., convolutional, recurrent, dense), and the use of skip connections or attention mechanisms, depending on the nature of your data and task.
		
		Hyperparameter Tuning: Experiment with different hyperparameter settings to optimize the performance of your model. This includes parameters such as learning rate, batch size, regularization techniques, and activation functions. Utilize techniques like cross-validation or grid search to systematically explore the hyperparameter space.
		
		Regularization and Overfitting: Incorporate regularization techniques, such as dropout, weight decay, or early stopping, to prevent overfitting. Monitor the training and validation loss to ensure your model generalizes well to unseen data.
		
		Choice of Loss Function: Select an appropriate loss function based on the nature of your task. For example, mean squared error (MSE) is commonly used for regression problems, while categorical cross-entropy is often used for multi-class classification tasks.
		
		Training Strategy: Decide on the training strategy for your model. This includes selecting an optimizer (e.g., Adam, SGD), defining the learning rate schedule, and determining the number of epochs or iterations for training. Monitor the training progress using metrics and visualize the learning curves to assess model convergence and performance.
		
		Model Evaluation: Evaluate your model's performance using appropriate evaluation metrics for your task, such as accuracy, precision, recall, F1-score, or mean absolute error (MAE). Additionally, consider cross-validation or using separate validation and test sets to assess the generalization performance of your model.
		
		
		Hardware and Computational Resources: Deep learning models are computationally intensive, so ensure you have access to suitable hardware (e.g., GPUs, TPUs) and sufficient computational resources to train and evaluate your model efficiently.
		
		
		Remember that deep learning models can be complex and require significant computational resources and expertise to develop effectively. It's crucial to iterate, experiment, and fine-tune your model based on empirical observations and domain knowledge to achieve the desired performance.}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\setcounter{subfigure}{0}
	\section{Case study: Delamination identification in CFRP}
	\subsection{An overview of guided waves}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	\begin{frame}{Motivations}
%		\begin{columns}[T]
%			\begin{column}[t]{.55\textwidth}
%				\begin{figure}[t]
%					%					\centering
%					%					\subfloat{\includegraphics[width=.7\textwidth]{Composite_advantages.png}}					
%					%					\\
%					\subfloat{\includegraphics[width=.85\textwidth]{delaminated_plate1.jpg}}
%				\end{figure}
%				\begin{tcolorbox}
%					\justifying\noindent\alert{Delamination detection} in its early stages can significantly help avoiding catastrophic events.
%				\end{tcolorbox}				
%			\end{column}
%			\begin{column}[t]{0.45\textwidth}
%				\begin{figure}[t]
%					\includegraphics[width=1\textwidth]{Crashes.png}
%				\end{figure}
%				\tiny {source: https://www.structuresinsider.com/post/the-difference-between-buckling-compression-shear}
%			\end{column}
%		\end{columns}
%	\end{frame}
%	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	\note
%	{
%		Composite laminates have a wide range of applications in various industries due to their good characteristics, such as:
%		high strength, low density, light weight, and resistance to fatigue and corrosion, among others.
%		
%		However, composite laminates may encounter some defects, such as cracks, fibre breakage, and debonding.
%		
%		In particular, composite laminates are more sensitive to damage in the form of delamination due to weak transverse tensile and interlaminar shear strengths.
%		
%		Therefore, delaminations can seriously decrease the performance of composite structures.
%		Accordingly, delamination detection in its early stages can significantly help to avoid catastrophic structural collapses.
%	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{frame}{Defects in composite laminates}
		\small
		Composite laminates can have different types of damage such as: \\
		\textbf{Cracks, fibre breakage, debonding, and \alert{delamination}.} \\ 
		\begin{columns}[T]
			\begin{column}[c]{0.5\textwidth}
				\begin{itemize}
					\footnotesize
					\item Delamination is a critical failure mechanism in laminated fibre-reinforced polymer matrix composites.
					\item Delamination is one of the most hazardous forms of the defects. 
					It leads to very catastrophic failures if not detected at early stages.
					\item Delamination can be invisible from the outside.
				\end{itemize}
			\end{column}
			\begin{column}[c]{0.45\textwidth}
				\begin{figure}
					\subfloat{\includegraphics[width=.95\textwidth]{delaminated_plate1.jpg}}
				\end{figure}
			\end{column}
		\end{columns}
		\begin{tcolorbox}
			\justifying\noindent\alert{Delamination detection} in its early stages can significantly help avoiding catastrophic events.
		\end{tcolorbox}
	\end{frame}
	\note{
		Composite laminates have a wide range of applications in various industries due to their good characteristics, such as:
		high strength, low density, light weight, and resistance to fatigue and corrosion, among others.
		
		However, composite laminates may encounter some defects, such as cracks, fibre breakage, and debonding.
		
		In particular, composite laminates are more sensitive to damage in the form of delamination due to weak transverse tensile and interlaminar shear strengths.
		
		Therefore, delaminations can seriously decrease the performance of composite structures.
		Accordingly, delamination detection in its early stages can significantly help to avoid catastrophic structural collapses.	
	}
	\begin{frame}{Waves used in non-destructive testing}
		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		Elastic wave propagation types depending on particle motion:
		\begin{itemize}
			\item  \alert{The longitudinal wave} is a compressional wave in which the particle motion is in the same direction as the propagation of the wave
			\item \alert{The shear wave} is a wave motion in which the particle motion is perpendicular to the direction of the propagation
			\item \alert{Surface (Rayleigh) waves} have an elliptical particle motion and travel across the surface of a material. Their velocity is approximately 90\% of the shear wave velocity of the material and their depth of penetration is approximately equal to one
			wavelength
			\item \alert{Plate (Lamb) waves} have a complex vibration occurring in materials where thickness is less than the wavelength of elastic wave introduced into it.
		\end{itemize}
	\end{frame}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	\setcounter{subfigure}{0}
%	\begin{frame}{Waves used in non-destructive testing Cont.}
%		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%		\begin{figure}
%			\subfloat{\animategraphics[autoplay,loop, controls,width=0.5\textwidth]{10}{figures/gif_figs/Longitudinal_wave/Longitudinal_wave-}{0}{35}}
%			\caption{\alert{Longitudinal wave} - plane pressure pulse wave}
%		\end{figure}
%		\tiny 
%		(source: https://nojigon.webs.upv.es/index.php)
%	\end{frame}
%	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	\setcounter{subfigure}{0}
%	\begin{frame}{Waves used in non-destructive testing Cont.}
%		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%		\begin{columns}[T]
%			\begin{column}[c]{0.32\textwidth}
%				\centering
%				\begin{figure}
%					\subfloat{\animategraphics[autoplay,loop, controls,width=0.95\textwidth]{10}{figures/gif_figs/SH_shear_wave/SH_shear-}{0}{39}}
%					\caption{\alert{Shear horizontal wave}}
%				\end{figure}			
%			\end{column}
%			\begin{column}[c]{0.32\textwidth}
%				\centering
%				\begin{figure}
%					\subfloat{\animategraphics[autoplay,loop, controls,width=0.95\textwidth]{10}{figures/gif_figs/SV_shear_wave/SV_shear-}{0}{39}}
%					\caption{\alert{Shear vertical wave}}
%				\end{figure}			
%			\end{column}
%			\begin{column}[c]{0.32\textwidth}
%				\centering
%				\begin{figure}
%					\centering
%					\subfloat{\animategraphics[autoplay,loop, controls,width=0.95\textwidth]{15}{figures/gif_figs/Raileigh_wave/Raileigh_wave-}{0}{67}}
%					\caption{\alert{Rayleigh waves}}		
%				\end{figure}
%			\end{column}	
%		\end{columns}
%		\tiny 
%		(source: https://nojigon.webs.upv.es/index.php)
%	\end{frame}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	\setcounter{subfigure}{0}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	\setcounter{subfigure}{0}
%	\begin{frame}{Waves used in non-destructive testing}
%		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%		\begin{figure}			
%			\centering
%			\subfloat{\animategraphics[autoplay,loop, controls,height=0.65\textheight]{20}{figures/gif_figs/love_wave/love_wave-}{0}{107}}
%			\caption{\alert{Love waves} (surface seismic waves) named after Augustus Edward Hough Love}		
%		\end{figure}			
%		\tiny 
%		(source: https://nojigon.webs.upv.es/index.php)
%	\end{frame}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\setcounter{subfigure}{0}
%	\begin{frame}{Lamb waves}
%		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%		\begin{alertblock}{Lamb waves}	
%			Lamb waves are plane waves propagating in thin plates.\\
%			Shear vertical waves in conjunction with longitudinal P waves interacts with plate surfaces resulting in complex wave mechanism which leads to creation of Lamb waves.
%		\end{alertblock}
%		%		Horace Lamb discovered these type of waves in 1917.
%		%		He derived theory and dispersion relations.
%		\begin{columns}[T]
%			\begin{column}{0.5\textwidth}
%				\centering
%				symmetric modes
%				\begin{equation*}
%					\frac{\tan(q h)}{\tan(p h)} = -\frac{4 k^2 p q}{\left(q^2 - k^2\right)^2}
%				\end{equation*}
%			\end{column}
%			\begin{column}{0.5\textwidth}
%				\centering
%				antisymmetric modes
%				\begin{equation*}
%					\frac{\tan(q h)}{\tan(p h)} = -\frac{\left(q^2 - k^2\right)^2}{4 k^2 p q}
%				\end{equation*}
%			\end{column}	
%		\end{columns}	
%		\centering
%		%		\(q=q(\omega,k), \quad p=p(\omega,k) \)
%		\begin{gather*}
%			\centering
%			p^2 = \frac{\omega^2}{c_{L}^2}-k^2,\ q^2 = \frac{\omega^2}{c_{S}^2}-k^2,\ k = \frac{2\pi}{\lambda},\ f=\frac{\omega}{2\pi}
%		\end{gather*}
%		\newline
%		\begin{gather*}
%			\centering
%			c_L=\sqrt{\frac{2\mu (1-\nu)}{\rho(1-2\nu)}},\ c_S=\sqrt{\frac{\mu}{\rho}}
%		\end{gather*}
%	\end{frame}
%	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	\note{
%		Lamb waves are a subset of guided waves that propagate between two parallel surfaces, such as in shell and plate structures discovered by Horace Lamb in 1917.
%		
%		Lamb waves are plane waves propagating in thin plates.\\
%		Shear vertical waves in conjunction with longitudinal P waves interacts with plate surfaces resulting in complex wave mechanism which leads to creation of Lamb waves.
%		
%		Lamb waves can be separated into symmetric modes and antisymmetric modes in terms of the surface particle motion to the mid-plane, which are described in characteristic Lamb wave equations:
%		
%		
%		where $h$ is the half-thickness of the plate, $k$ is the wavenumber, $\omega$ is the angular frequency, and $\lambda$ is the wavelength. 
%		\(cL\) and \(cS\) are the velocities of longitudinal and transverse
%		waves, respectively.
%		
%		where \(\rho\) is the density, \(\mu\) is the shear modulus, and \(\nu\) is Poisson's ratio of the medium.
%	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\setcounter{subfigure}{0}
	\begin{frame}{Lamb waves modes}
		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
		\begin{columns}[T]
			\begin{column}{0.3\textwidth}
				\centering
				\begin{figure}
					\animategraphics[autoplay,loop,width=1\textwidth]{10}{figures/gif_figs/S0_mode/S0_mode-}{0}{67}
					\caption{Fundamental symmetric, S0, \alert{Lamb wave} mode (in-plane motion)}
				\end{figure}
			\end{column}
			\begin{column}{0.3\textwidth}
				\centering
				\begin{figure}
					\animategraphics[autoplay,loop,width=1\textwidth]{10}{figures/gif_figs/A0_mode/A0_mode-}{0}{67}
					\caption{Fundamental antisymmetric, A0, \alert{Lamb wave} mode (out-of-plane motion)}
				\end{figure}
			\end{column}
			\hfill
			\begin{column}{0.37\textwidth}
				\begin{itemize}
					\item \textcolor{blue}{Travel within guides for long distances}
					\item \textcolor{blue}{Can propagate in complex structures}
					%						\item \textcolor{blue}{High speeds in metals and composites}
					\item \textcolor{blue}{Can be automated using software}
				\end{itemize}
				\textbf{Lamb waves are a promising global NDE solution for SHM}
			\end{column}
		\end{columns}	
		\tiny 
		(source: https://nojigon.webs.upv.es/index.php)
	\end{frame}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\note{
		Lamb waves are a subset of guided waves that propagate between two parallel surfaces, such as in shell and plate structures discovered by Horace Lamb in 1917.
		
		Lamb waves are plane waves propagating in thin plates.\\
		Shear vertical waves in conjunction with longitudinal P waves interacts with plate surfaces resulting in complex wave mechanism which leads to creation of Lamb waves.
		
		Lamb waves can be separated into symmetric modes and antisymmetric modes in terms of the surface particle motion to the mid-plane
		The animation on the left shows the fundamental symmetric \(S_0\) mode of the Lamb wave.
		
		We can see the in-plane motion of the particles.
		Whereas the second animation shows the fundamental antisymmetric \(A_0\) mode of the Lamb wave.
		We can see the out-of-plane motion of the particles.
		
		Furthermore, Lamb wave has some good characteristics that make it a promising solution for SHM as it:\\
		1. Can travel within guides for long distances. \\
		2. Can propagate in complex structures. \\
	}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\section*{Experimental measurements}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{frame}{Contact v.s. non-contact methods} 
		\begin{columns}[T]
			\begin{column}[c]{0.38\textwidth}
				%				\textbf{Ultrasonic waves}	
				%				\begin{itemize}
					%					\item Frequency range: 2 MHz - 200 MHz
					%					\item Wavelength \(\lambda << h\) thickness 
					%					\item shorter wavelengths
					%				\end{itemize}
				\textbf{Guided waves}	
				\begin{itemize}
					\item Typical frequency range: \\ 10 kHz - 1 MHz
					\item Wavelength \(\lambda > h\) thickness 
					%					\item longer wavelengths
				\end{itemize}
			\end{column}
			\begin{column}[c]{0.6\textwidth}				
				\begin{figure}
					\includegraphics[height=0.8\textheight]{local_ultrasonic.png}
				\end{figure}
			\end{column}		
		\end{columns}	
	\end{frame}
	\note{
		Many conventional methods have been used for a long time in the industry for damage inspection, such as visual inspection, Eddy current, dye penetration, acoustic emission, and local ultrasonic testing.
		For the local ultrasonic testing, the frequency is in the range of 2 MHz–200 MHz, and the wavelength is way smaller than the thickness of the structure.
		There are different methods for ultrasonic testing, such as:
		pulse echo, in which we have a probe that generates ultrasonic waves that propagate through the thickness of a structure, and then the reflected waves from the bottom surface are captured by the same probe.
		The other ultrasonic method is called the "through-transmission method."
		There are two probes: one for transmitting on one side and the other for receiving on the other side.
		As a result, these methods are local, but they have some drawbacks:
		Labor-intensive, time-consuming, professional, and experienced personnel are required; complex structures may need to be disassembled.
		On the other hand, guided wave testing is a promising global NDE solution for SHM.
		The typical frequency range of guided waves is 10 kHz to 1 MHz, and their wavelength is larger than the thickness of the investigated structure.
		Guided waves can be generated using a piezoelectric actuator and registered by an array of piezoelectric sensors. 
		Alternatively, the propagation of guided Lamb waves can be registered using a scanning laser Doppler vibrometer to produce a full wavefield scan.		 
	}
	
	\begin{frame}[t]{SLDV measurements: Setup}
		\begin{columns}[T]
			\column{0.5\textwidth}
			\begin{figure}
				\includegraphics[width=0.8\textwidth]{wibrometr-laserowy-1d_small-description.png}
			\end{figure}
			\column{0.5\textwidth}
			\begin{enumerate}
				\item Signal generator: TTI 1241 
				\item Amplifier: Piezo Systems EPA-104-230 $\pm$200 Vp
				\item Specimen
				\item Scanning head: Polytec PSV-400
				\item DAQ system: Polytec
			\end{enumerate}
		\end{columns}
		{\small
			Measurements were taken on a uniform grid of \textbf{333$\times$333 points}.\\
			Excitation in the form of Hann windowed sine signal of carrier frequency \textbf{50 kHz} was applied to piezoelectric transducer.}
		\end{frame}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\note
	{
		In this slide, I present the experimental setup for data acquisition.
		
		The specimen was excited using a Piezoelectric transducer placed at the centre of the plate.
		
		The scanning head is used to acquire the full wavefields of guided wave propagation from the bottom surface of the plate.	
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{frame}[t]{Composite specimen}
		\begin{columns}[T]
			\begin{column}[c]{0.6\textwidth}
				\begin{itemize}
					\item 16 layers set at the same angle 
					\item carbon: Prepreg GG 205 P (fibres Toray FT 300 - 3K 200 tex), $E=230$ GPa
					\item epoxy resin: IMP503Z-HT by Impregnatex Compositi 						
					\item density: 1522.4~kg/m\textsuperscript{3}
					\item dimensions: 500$\times$500$\times$3.9 mm
				\end{itemize}			
			\end{column}
			\begin{column}[c]{0.4\textwidth}
				\begin{figure}
					\centering
					\includegraphics[width=.6\textwidth]{weave-1.jpg}
					\caption{Plain weave fabric}
				\end{figure}
			\end{column}			
		\end{columns}
	\end{frame}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\note{
		The specimen used for experimental evaluation are carbon fibre reinforcement polymer (CFRP) composed of 16 layers of plain weave fabric with the following characteristics:
		
		The specimen dimension is \(500 \times 500\)~mm with a thickness of 3.9~mm.
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{frame}[t]{Specimens with defects}
		\vspace{-0.5cm}
		\begin{columns}[T]
			\column{0.5\textwidth}
			\begin{figure}
				\includegraphics[width=0.9\textwidth]{plate_multi_delam_arrangement_large_fonts.png}
			\end{figure}
			\column{0.5\textwidth}
			\begin{figure}
				\includegraphics[width=0.75\textwidth]{plate_single_delam_arrangement_large_fonts.png}
			\end{figure}
		\end{columns}
	\end{frame}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\note
	{	
		\footnotesize
		The specifications of the CFRP specimens used for evaluating the developed deep-learning models are shown in this slide.
		
		Specimens with multiple delaminations are presented on the left.
		In which there are three specimens with multiple delaminations located at the same distance from the centre of the plate, and placed at different thicknesses. 
		
		The figure on the right shows the specimen with a single delamination placed at the half thickness, and the total thickness of the specimen is 3.5 mm.
		
		Furthermore, all measurements with SLDV were conducted from the bottom surface of the plate.		
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\subsection{Synthetic dataset generation}
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\setcounter{subfigure}{0}
	\begin{frame}{Dataset description}
		\begin{columns}[T]
			\begin{column}[t]{0.35\textwidth}
					\begin{itemize}
						\item 475 delamination scenarios
						\item CFRP is made of 8-layers
						\item Delamination modelled between the 3rd and 4th layer
						\item Delamination size min 10 mm, max  40 mm
						\item \textbf{3-months of computing}
					\end{itemize}
			\end{column}
			\begin{column}[t]{0.2\textwidth}
				\begin{figure}[t]
					\centering
					\subfloat[Delamination placement]{\includegraphics[width=0.95\textwidth]{delamination_placement.png}}
				\end{figure}
			\end{column}
			\begin{column}[t]{0.45\textwidth}
					\begin{figure}[t]					
						\centering						
						\subfloat[Delamination orientation]{\includegraphics[width=0.95\textwidth]{figure1.png}}					
					\end{figure}
			\end{column}
		\end{columns}
	\end{frame}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\note{
		The synthetically generated dataset has 475 delamination cases.
		It was assumed that the composite laminate is made of eight layers with a total thickness of 3.9 mm. 
		The delamination was modeled between the third and fourth layers.
		The delamination geometrical size major and minor axes were randomly selected from interval \([10mm,\ 40mm]\). 
		The computation of the dataset took about three months.
	}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{frame}{The time domain spectral element method}
		\begin{columns}[T]
			\begin{column}{0.47\textwidth}
				\begin{itemize}
					\item Mindlin-Reissner plate theory
					\item Splitting elements and nodes at delamination
					\item GMSH software was used for meshing quads then converted to spectral elements
				\end{itemize}	
				\begin{figure}
					\subfloat{\includegraphics[width=0.9\textwidth]{shell.png}}	
				\end{figure}
			\end{column}
			\begin{column}{0.47\textwidth}	
				\begin{figure}
					\animategraphics[controls,autoplay,loop,width=0.9\textwidth]{1}{/gif_figs/mesh/m1_rand_single_delam_}{1}{20}
				\end{figure}	
			\end{column}
		\end{columns}	
	\end{frame}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\note{
		To train the supervised deep learning models, a synthetic dataset of propagating waves in carbon fibre-reinforced composite plates was computed by applying the Mindlin-Reissner plate theory and using the parallel implementation of the time domain spectral element method.
		
		For each case, single delamination was modeled by using the method of splitting nodes between appropriate spectral elements.
		
		Essentially, the dataset resembles the particle velocity measurements at the bottom surface of the plate acquired by the SLDV in the transverse direction as a response to the piezoelectric (PZT) excitation at the centre of the plate.		 
	}
	\setcounter{subfigure}{0}
	\begin{frame}{Training Sample case}
		\begin{columns}[T]
			\begin{column}[c]{.32\textwidth}
				\begin{figure}
					\centering
					\animategraphics[autoplay,loop,width=0.95 \textwidth]{16}{figures/gif_figs/7_output/flat_shell_Vz_7_500x500bottom-}{1}{512}
					\caption{Full wavefield $s(x,y,t_k)$}
				\end{figure}
			\end{column}
			\begin{column}[c]{.32\textwidth}
				\begin{figure}
					\centering
					\includegraphics[width=0.95 \textwidth]{RMS_flat_shell_Vz_7_500x500bottom.png}
					\caption{RMS image $\hat{s}(x,y)$}
				\end{figure}
			\end{column}
			\begin{column}[c]{.32\textwidth}
				\begin{figure}
					\centering
					\includegraphics[width=0.95 \textwidth]{m1_rand_single_delam_7.png}
					\caption{Ground truth (label)}
				\end{figure}
			\end{column}
		\end{columns}
		The RMS is defined as:
		\begin{equation*}
			\hat{s}(x,y) = \sqrt{\frac{1}{N}\sum_{k=1}^{N}s(x,y,t_k)^2} 
			\label{eqn:rms} 
		\end{equation*}
	\end{frame}
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\note{
		In this slide, I present a training scenario.
		The animation on the left represents the full wavefield frames, where x and y are the point coordinates, and tk is the time step.
		
		The output of applying the root mean square formula to the full wavefield is shown in the middle figure.
		
		The ground truth label that represents the delamination location and shape is shown on the right.
	}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Delamination identification in CFRP}
	\setcounter{subfigure}{0}
	\begin{frame}{Computer vision}
	\begin{columns}[T]
		\begin{column}[c]{0.27\textwidth}
			\justifying
			\alert {\textbf{Computer vision}} is a field of AI that enables computers and systems to derive meaningful information from digital images, videos and other visual inputs. 
		\end{column}
		\quad
		\begin{column}[c]{0.7\textwidth}
			\begin{figure}
				\centering
				\includegraphics[width=1\textwidth]{computer_vision_tasks.png}
			\end{figure}
		\end{column}
	\end{columns}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\note{
	We now reach an essential concept: computer vision. 
	Computer vision is a subcategory of artificial intelligence that enables computers to obtain meaningful information from visual inputs.
	
	Computer vision has three hierarchical levels: 
	
	The first level performs a classification for the whole input and predicts one output.
	
	The second level classifies and locates the object that we are looking for. 
	
	The ultimate level of computer vision is to perform Pixel-wise segmentation (or semantic segmentation), in which each pixel in the input image is classified into its corresponding class.	
	
}
\begin{frame}{Delamination identification approaches}
	\begin{columns}[T]
		\begin{column}[c]{0.47\textwidth}
			\centering
			\textbf{One-to-one \\image-based approach (RMS)} 
			\begin{figure}
				\centering
				\captionsetup{justification=centering}				
				\subfloat[Single input (image)]{\includegraphics[width=.45\textwidth]{RMS_flat_shell_Vz_381_500x500bottom.png}}\quad
				\subfloat[Single output]{\includegraphics[width=.45\textwidth]{GCN_381.png}}
			\end{figure}
		\end{column}
		\hfill
		\begin{column}[c]{0.47\textwidth}
			\centering
			\textbf{Many-to-one \\animation-based approach}
			\begin{figure}
				\centering
				\captionsetup{justification=centering}					
				\subfloat[Multiple frames (animation)]{\animategraphics[autoplay,loop,width=.45\textwidth]{16}{figures/gif_figs/381_output/output_381-}{1}{512}}\quad
				\subfloat[Single output]{\includegraphics[width=.45\textwidth]{GCN_381.png}}
			\end{figure}
		\end{column}	
	\end{columns}		
\end{frame}	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\note{
	In part, I will present deep learning approaches for delamination identification in composite laminates.
	
	It is important to note how these models perform in an end-to-end fashion.
	
	Accordingly, two approaches for delamination identification based on their inputs were adopted:
	
	\begin{itemize}
		\item the first one is the One-to-one approach (takes one input that is the root mean squared of the full wavefield) and produces one output of damage map)
		\item the second one is the Many-to-one approach (takes animation of Lamb waves propagation as a sequence of frames and produces one output as damage map).
	\end{itemize}	
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	\subsection{Developed DL models}
%\begin{frame}{Common deep learning architectures}
%	
%	\begin{column}[t]{0.45\textwidth}
	%		\textbf{RMS based}\\
	%		\begin{itemize}
		%			\item Convolutional neural networks (CNN)
		%			\item Fully convolutional network (FCN)
		%		\end{itemize}
	%	\end{column}
%	\hfill
%	\begin{column}[t]{0.45\textwidth}
	%		\textbf{Full wavefield frames}\\
	%		\begin{itemize}
		%			\item Recurrent neural network (RNN)
		%			\item Long short-term memory (LSTM)
		%			\item ConvLSTM
		%		\end{itemize}
	%	\end{column}
%\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Developed model for delamination identification}
	\begin{columns}[T]
		\begin{column}[t]{0.45\textwidth}
			\begin{block}{RMS based models}
				\begin{itemize}
					\item VGG 16 encoder-decoder
					\item Res-UNet					
					\item FCN-DenseNet
					\item PSPNet
					\item GCN
				\end{itemize}				
			\end{block}
		\end{column}
		\hfill
		\begin{column}[t]{.50\textwidth}
			\begin{block}{Full wavefield frames based model}					
				\begin{itemize}
					\item Autoencoder ConvLSTM (Convolutional~Long~Short-Term Memory)
				\end{itemize}									
			\end{block}
		\end{column}
	\end{columns}
\end{frame}	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\note{
	
	The developed models based on the RMS image of the full wavefield input are:
	The residual UNet, VGG16 encoder-decoder, fully convolutional dense network (FCN-DenseNet), pyramid scene parsing network (PSPNet), and the global convolutional neural network (GCN)
	
	And the developed model based on the animation of full wavefield frames is Autoencoder ConvLSTM.		
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{RMS based models}	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{subfigure}{0}
\begin{frame}{Developed RMS based models}
	\begin{figure}
		\includegraphics[width=0.95\textwidth]{Developed_rms_models.png}
	\end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{subfigure}{0}	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\note{
	Here, I present the architectures of all the deep learning models I've created that are based on the RMS image of the full wavefield.
	
	For further details about these models, kindly, I advise you to return to my thesis and my published papers. 
	%		Both the Res-UNet and VGG16 encoder-decoders are autoencoders.
	%		The main difference between them is the additional skip connections that were added to Res-Unet at the encoder and decoder levels. \\
	%		FCN-DenseNet applies an encoder-decoder scheme with skip connections between the encoder and the decoder paths.
	%		The main component in FCN-DenseNet is the dense block. 
	%		The dense block is constructed from a varying number of convolutional layers. 
	%		The purpose of the dense block is to concatenate feature maps of a layer with its output to emphasize spatial details information. \\		
	%		The idea of PSPNet is to provide adequate global contextual information for pixel-level scene parsing by concatenating the local and global features together. 
	%		Hence, a spatial pyramid pooling module was introduced to perform four different pooling levels with four different pool sizes. 
	%		In this way, the pyramid pooling module can capture contextual features at different scales.\\		
	%		GCN addresses the importance of having large kernels at the convolution operations for both localization and classification tasks for semantic segmentation.
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{subfigure}{0}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Full wavefield frames based model}
\begin{frame}{Autoencoder ConvLSTM}
	\begin{figure}[ht!]
		\centering
		\includegraphics[width=1\textwidth]{figure3.png}
	\end{figure}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\note{
	Here, I present the developed Autoencoder-ConvLSTM model, which takes a sequence of consecutive full wavefield frames of guided wave propagation as an input.
	
	To reduce the training complexity, I used a certain number of frames after the initial interaction of guided waves with the damage, as the delamination location is known.
	
	These selected frames, which contain the required features regarding the delamination shape and location, are fed into an encoder-decoder model at once using a time-distributed layer.
	
	Then, the output of the decoder is forwarded into the ConvLSTM layer, which handles time series data by learning long-term spatio-temporal features.
	
	For real-life situations where the damage location is unknown,  the full wavefield is tested through a sliding window that produces an intermediate output at a time.
	Finally, the root-mean-square formula is applied to all intermediate predictions to obtain the RMS damage map.		
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Evaluation metrics for delamination identification}
	\begin{columns}[T]
		\begin{column}[c]{0.45\textwidth}
			For evaluating delamination identification
			\begin{itemize}
				\item Intersection over Union (IoU): 
				\begin{equation*}
					\textup{IoU}=\frac{Intersection}{Union}=\frac{\hat{Y} \cap Y}{\hat{Y} \cup Y}
					\label{eqn:iou}
				\end{equation*}
				\item Percentage area error $\epsilon$:
				\begin{equation*}
					\epsilon=\frac{|A-\hat{A}|}{A} \times 100\%
					\label{eqn:mean_size_error}
				\end{equation*}
			\end{itemize}
		\end{column}
		\begin{column}[c]{0.45\textwidth}
			\begin{figure}
				\centering
				\includegraphics[width=1.0\textwidth]{IoU_figure.png}		
			\end{figure}
		\end{column}
	\end{columns}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\note{
	Now, to evaluate the developed models for delamination identification, 
	I used two metrics:
	The first metric is the mean intersection over union (also known as the Jaccard index), which calculates the area of intersection between actual and predicted values divided by the union of them.
	The second metric is the percentage area error \(\epsilon\), which calculates the percentage of the difference between actual and predicted areas. 
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Evaluation: Numerical cases}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Numerical test cases RMS based models (GCN model)}
	\begin{columns}[T]
		\begin{column}[c]{0.32\textwidth}
			\begin{figure}[c]
				\centering
				\captionsetup{justification=centering}					
				\animategraphics[controls,width=.9\textwidth]{8}{figures/gif_figs/456/intermediate_output-}{0}{82}
				\caption{\(1^{st}\) numerical case, IoU=0.71}
			\end{figure}
		\end{column}
		\hfill
		\begin{column}[c]{0.32\textwidth}
			\begin{figure}[c]
				\centering
				\captionsetup{justification=centering}					
				\animategraphics[controls,width=.9\textwidth]{8}{figures/gif_figs/438/intermediate_output-}{0}{82}
				\caption{\(2^{nd}\) numerical case, IoU=0.72}
			\end{figure}
		\end{column}
		\hfill
		\begin{column}[c]{0.32\textwidth}
			\begin{figure}[c]
				\centering
				\captionsetup{justification=centering}
				
				\animategraphics[controls,width=.9\textwidth]{8}{figures/gif_figs/397/intermediate_output-}{0}{82}
				\caption{\(3^{rd}\) numerical case, IoU=0.86}
			\end{figure}					
		\end{column}
	\end{columns}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\note{
	This slide shows three numerical samples tested with the GCN model.
	For the first case, the delamination is barely visible by the naked eye, yet the model could identify the delamination with IoU = 0.71.
	For the second case, the IOU = 0.72, and for the third case, the IOU = .86
	Additionally, each animation shows all extracted feature maps from the RMS image input until we get the final prediction of the damage map.
	It's important to notice that GCN can identify the delamination with high accuracy and noise free.			
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	\begin{frame}{RMS based: Analysis of numerical cases}
	%		\begin{columns}[T]
		%%				\tiny
		%%				\begin{column}[c]{0.48\textwidth}
			%%					%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			%%					\begin{table}[ht!]
				%%						\centering
				%%						\caption{Evaluation metrics of the three numerical cases.}
				%%						\label{tab:RMS_num_cases}
				%%						\begin{tabular}{cccccc}
					%%							\toprule[1.5pt]
					%%							\multirow{2}{*}{Model} & \multirow{2}{*}{case number} & \multicolumn{1}{c}{\multirow{2}{*}{A [mm\textsuperscript{2}]}} & \multicolumn{3}{c}{Predicted output} \\ 
					%%							\cmidrule(lr){4-6} & & & \multicolumn{1}{c}{IoU} & \multicolumn{1}{c}{\(\hat{A}\) [mm\textsuperscript{2}]} & \(\epsilon\) \\
					%%							\midrule
					%%							\multirow{3}{*}{Res-UNet} 							
					%%							& 1 & 257 & \multicolumn{1}{c}{0.45} & \multicolumn{1}{c}{143} & \(44.36\%\) \\ 
					%%							& 2 & 105 & \multicolumn{1}{c}{0.67} & \multicolumn{1}{c}{88} & \(16.19\%\) \\ 
					%%							& 3 & 537 & \multicolumn{1}{c}{0.80} & \multicolumn{1}{c}{478} & \(10.99\%\) \\ 
					%%							\midrule
					%%							\multirow{3}{*}{VGG16 encoder-decoder} 
					%%							& 1 & 257 & \multicolumn{1}{c}{0.69} & \multicolumn{1}{c}{203} & \(21.01\%\) \\ 
					%%							& 2 & 105 & \multicolumn{1}{c}{0.75} & \multicolumn{1}{c}{117} & \(11.43\%\) \\ 
					%%							& 3 & 537 & \multicolumn{1}{c}{0.65} & \multicolumn{1}{c}{385} & \(28.31\%\) \\ 
					%%							\midrule
					%%							\multirow{3}{*}{FCN-DenseNet} 
					%%							& 1 & 257 & \multicolumn{1}{c}{0.52} & \multicolumn{1}{c}{505} & \(96.50\%\) \\ 
					%%							& 2 & 105 & \multicolumn{1}{c}{0.66} & \multicolumn{1}{c}{118} & \(12.38\%\) \\ 
					%%							& 3 & 537 & \multicolumn{1}{c}{0.72} & \multicolumn{1}{c}{815} & \(51.77\%\) \\ 
					%%							\midrule
					%%							\multirow{3}{*}{PSPNet} 
					%%							& 1 & 257 & \multicolumn{1}{c}{0.00} & \multicolumn{1}{c}{0} & \(-\%\) \\ 
					%%							& 2 & 105 & \multicolumn{1}{c}{0.44} & \multicolumn{1}{c}{156} & \(48.57\%\) \\ 
					%%							& 3 & 537 & \multicolumn{1}{c}{0.77} & \multicolumn{1}{c}{610} & \(13.59\%\) \\ 
					%%							\midrule
					%%							\multirow{3}{*}{GCN} 
					%%							& 1 & 257 & \multicolumn{1}{c}{0.71} & \multicolumn{1}{c}{215} & \(16.34\%\) \\ 
					%%							& 2 & 105 & \multicolumn{1}{c}{0.72} & \multicolumn{1}{c}{177} & \(68.57\%\) \\ 
					%%							& 3 & 537 & \multicolumn{1}{c}{0.86} & \multicolumn{1}{c}{523} & \(2.61\%\) \\ 
					%%							\bottomrule[1.5pt]
					%%						\end{tabular}	
				%%					\end{table}
			%	 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
			%% \end{column}
		%%		\hfill
		%		\begin{column}[c]{0.9\textwidth}
			%			\begin{table}[ht!]
				%				\centering
				%				\caption{Analysis of numerical cases.}
				%				\label{tab:table_all_numerical_cases}	
				%				\begin{tabular}{lcc}
					%					\toprule[1.5pt]
					%					Model & mean IoU & max IoU \\ 
					%					\midrule 
					%					Res-UNet & \(0.66\) & \(0.89\) \\ 
					%					VGG16 encoder-decoder & \(0.57\) & \(0.84\) \\ 
					%					FCN-DenseNet & \(0.68\) & \(0.92\) \\ 
					%					PSPNet & \(0.55\) & \(0.91\) \\ 
					%					GCN & \textbf{\(0.76\)} & \textbf{\(0.93\)} \\ 
					%					\bottomrule[1.5pt]
					%				\end{tabular}
				%			\end{table}
			%		\end{column}
		%		\end{columns}
	%	\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	\note{
	%		The table presents the mean and maximum values calculated for the previously unseen numerical test set for all RMS-based models. 
	%		It also shows that all models have a relatively high value, indicating their ability to detect and localize the delamination.
	%		However, the best performance was achieved by the GCN model.
	%	}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Numerical test cases animation of Lamb waves}
	\setcounter{subfigure}{0}
	\only<1>{
		\begin{alertblock}{First test case}
			\begin{figure}
				\centering
				\captionsetup{justification=centering}
				\subfloat[Full wavefield (512 frames)]{\animategraphics[autoplay,loop,height=3cm,keepaspectratio]{32}{figures/gif_figs/381_output/output_381-}{1}{512}}\quad
				\subfloat[Intermediate outputs]{\animategraphics[autoplay,loop,height=3cm,keepaspectratio]{31}{figures/gif_figs/Numerical_case_381/num_case_381_frame_num-}{0}{487}}\quad
				\subfloat[RMS (damage map)]{\includegraphics[height=3.05cm,keepaspectratio]{figures/RMS_Ijjeh_num_case_381.png}}\quad
				\subfloat[Binary RMS, IoU= 0.88]{\includegraphics[height=3cm,keepaspectratio]{figures/Binary_RMS_Ijjeh_num_case381_.png}}\quad
			\end{figure}
	\end{alertblock}}
	\setcounter{subfigure}{0}
	\only<2>{
		\begin{alertblock}{Second test case}
			\begin{figure}
				\centering
				\captionsetup{justification=centering}
				\subfloat[Full wavefield (512 frames)]{\animategraphics[autoplay,loop,height=3cm,keepaspectratio]{32}{figures/gif_figs/385_output/output_385-}{1}{512}}\quad		
				\subfloat[Intermediate outputs]{\animategraphics[autoplay,loop,height=3cm,keepaspectratio]{31}{figures/gif_figs/Numerical_case_385/num_case_385_frame_num-}{0}{487}}\quad			
				\subfloat[RMS (damage map)]{\includegraphics[height=3.05cm,keepaspectratio]{figures/RMS_Ijjeh_num_case_385.png}}\quad
				\subfloat[Binary RMS, IoU= 0.58]{\includegraphics[height=3cm,keepaspectratio]{figures/Binary_RMS_Ijjeh_num_case385_.png}}
			\end{figure}
	\end{alertblock}}
	\setcounter{subfigure}{0}
	\only<3>{
		\begin{alertblock}{Third test case}
			\begin{figure}
				\centering
				\captionsetup{justification=centering}
				\subfloat[Full wavefield (512 frames)]{\animategraphics[autoplay,loop,height=3cm,keepaspectratio]{32}{figures/gif_figs/394_output/output_394-}{1}{512}}\quad
				\subfloat[Intermediate outputs]{\animategraphics[autoplay,loop,height=3cm,keepaspectratio]{31}{figures/gif_figs/Numerical_case_394/num_case_394_frame_num-}{0}{487}}\quad
				\subfloat[RMS (damage map)]{\includegraphics[height=3.05cm,keepaspectratio]{figures/RMS_Ijjeh_num_case_394.png}}\quad
				\subfloat[Binary RMS, IoU= 0.8]{\includegraphics[height=3cm,keepaspectratio]{figures/Binary_RMS_Ijjeh_num_case394_.png}}
			\end{figure}
	\end{alertblock}}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\note{
	In the following three slides, I will present three numerical samples evaluated with the Autoencoder ConvLSTM model.
	
	Animation (a) shows the full wavefield of 512 frames as an input to the model.
	Figure (b) shows the RMS damage map for all intermediate predictions,
	and figure (c) shows the binary RMS with IoU = 0.88.
	The second case is more complex, as the delamination is near the corner of the plate.
	In this case, the IoU is 0.58.
	The third case is also complex, as the delamination is located near the plate edge.
	In this case, the IoU is 0.8.		
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	\begin{frame}{Animation based: Analysis of numerical cases}
	%		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%		\begin{table}[!h]
		%			\centering
		%			\caption{Evaluation metrics of the three numerical cases.}
		%			\begin{tabular}{ccccc}
			%				\toprule[1.5pt]
			%				\multirow{2}{*}{case number} & \multicolumn{1}{c}{\multirow{2}{*}{A [mm\textsuperscript{2}]}} & \multicolumn{3}{c}{Predicted output} \\ 
			%				\cmidrule(lr){3-5} & & \multicolumn{1}{c}{IoU} & \multicolumn{1}{c}{\(\hat{A}\) [mm\textsuperscript{2}]} & \(\epsilon\) \\
			%				\midrule
			%				1 & 763 & \multicolumn{1}{c}{0.88} & \multicolumn{1}{c}{735} & \(3.67\%\) \\ 
			%				2 & 388 & \multicolumn{1}{c}{0.58} & \multicolumn{1}{c}{248} & \(36.08\%\) \\ 
			%				3 & 297 & \multicolumn{1}{c}{0.80} & \multicolumn{1}{c}{280} & \(5.72\%\) \\			 					
			%				\bottomrule[1.5pt]
			%			\end{tabular}	
		%			\label{tab:num_cases}
		%		\end{table}			
	%	\end{frame}
%	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	\note{
	%		 The shown table presents the evaluation metrics for the autoencoder ConvLSTM model regarding the three numerical cases shown in the previous slide.
	%		 
	%		 The table gathers the actual delamination area, predicted delamination area, intersection over union IoU, and percentage area error \(\epsilon\) to each case.
	%	}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection*{Evaluation: Experimental cases}	
\setcounter{subfigure}{0}		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Experimental results: RMS image-based (Single delamination)}
	\begin{columns}[T]
		\begin{column}[t]{.25\textwidth}
			\begin{figure}[ht!]
				\centering
				\captionsetup{justification=centering}
				\includegraphics[height=.35\textheight]{ERMS_with_label.png}
				\caption{ERMS \& label}
			\end{figure}
			\justifying
			\tiny
			Kudela, P., Radzienski, M. and Ostachowicz, W., 2018. \textbf{Impact induced damage assessment by means of Lamb wave image processing}. \textit{Mechanical Systems and Signal Processing}, 102, pp.23-36.
		\end{column}
		\begin{column}[t]{0.5\textwidth}
			\begin{block}{Adaptive wavenumber filtering}
				\centering
				\footnotesize
				IoU=$0.401$
				\begin{figure}[ht!]
					\centering
					\captionsetup{justification=centering}
					\subfloat{\includegraphics[height=.35\textheight]{ERMSF_CFRP_teflon_3o_375_375p_50kHz_5HC_x12_15Vpp.png}}
					\quad
					\subfloat{\includegraphics[height=.35\textheight]{Binary_ERMSF_CFRP_teflon_3o_375_375p_50kHz_5HC_x12_15Vpp.png}}
				\end{figure}
			\end{block}					
		\end{column}		
		\begin{column}[t]{0.25\textwidth}
			\begin{alertblock}{DL approach: GCN}
				\centering
				\footnotesize
				IoU\(=0.723\)
				\begin{figure}[ht!]	
					\centering				
					\subfloat{\includegraphics[height=.35\textheight]{Fig_GCN_7.png}}
				\end{figure} 	
			\end{alertblock}				
		\end{column}
	\end{columns}	
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\note{
	In this slide, I present the experimental results of a specimen with a single delamination using the RMS-based model, the GCN.
	Additionally, I compared my developed model with the adaptive wavenumber filtering technique for damage imaging, which is a conventional signal processing technique.
	Figure (a) shows the ERMS with a label depicting the delamination. 
	Figure (b) shows the result of applying the adaptive wavenumber filtering method, and Figure (c) shows its binary output with IoU=0.401.
	Figure (d) shows the predicted damage map by the GCN model with IoU = 0.723.
	As shown, the deep learning model surpasses the conventional signal processing approach.
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	\setcounter{subfigure}{0}
%	\begin{frame}{RMS based: Analysis of experimental case}
	%		\begin{table}[!ht]
		%			\centering
		%			\caption{Evaluation metrics of the experimental case.}
		%			\label{tab:rms_exp_case}
		%			\begin{tabular}{lc}
			%				\toprule[1.5pt]
			%				Model & IoU	\\			
			%				\midrule
			%				Res-UNet & 0.58 \\ 
			%				VGG16 encoder-decoder & 0.62 \\ 
			%				FCN-DenseNet & 0.54 \\ 
			%				PSPNet & 0.49 \\ 
			%				GCN & 0.72\\ 
			%				\bottomrule[1.5pt]
			%			\end{tabular}		
		%		\end{table}
	%%			\begin{table}[!ht]
		%%				\centering
		%%				\caption{Evaluation metrics of the experimental case.}
		%%				\label{tab:rms_exp_case}
		%%				\begin{tabular}{l@{\ }cccc}
			%%					\toprule
			%%					\multicolumn{1}{l}{Model} & \multicolumn{1}{c}{A [mm\textsuperscript{2}]} & \multicolumn{3}{c}{Predicted output} \\ 
			%%					\cmidrule(lr){3-5} & & \multicolumn{1}{c}{IoU} & \multicolumn{1}{c}{\(\hat{A}\) [mm\textsuperscript{2}]} & \(\epsilon\) \\ \midrule
			%%					Res-UNet & \multicolumn{1}{c}{\multirow{5}{*}{210}} & \multicolumn{1}{c}{0.58} & \multicolumn{1}{c}{323} & \(53.8\%\) \\ 
			%%					VGG16 encoder-decoder & & \multicolumn{1}{c}{0.62} & \multicolumn{1}{c}{320} & \(52.4\%\) 
			%%					\\ 
			%%					FCN-DenseNet & & \multicolumn{1}{c}{0.54} & \multicolumn{1}{c}{386} & \(83.8\%\) \\ 
			%%					PSPNet & & \multicolumn{1}{c}{0.49} & \multicolumn{1}{c}{580} & \(176.2\%\) 
			%%					\\ 
			%%					GCN & & \multicolumn{1}{c}{0.72} & \multicolumn{1}{c}{309} & \(47.1\%\) 
			%%					\\ 
			%%					\bottomrule
			%%				\end{tabular}		
		%%			\end{table}
	%	\end{frame}
%	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%	\note{
	%		The table shows the IoU values for all developed RMS based models for the single delamination specimen.
	%		
	%		Similarly to the numerical dataset, the best accuracy was achieved by using GCN.
	%		
	%%		As shown, the models are capable of precise detection and localisation of the delamination. 
	%%		We can see that the models can identify the delamination with almost free noise indicating the models are capable of generalising and detecting the delamination on previously unseen data. 
	%	}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{subfigure}{0}
\begin{frame}{Experimental results: Full wavefield based (Single delamination)}		
	\begin{alertblock}{DL approach}
		IoU= $0.41$% and $\epsilon=71.56\%$ 
		\begin{figure}[ht!]
			\centering
			\subfloat[Input]{\animategraphics[autoplay,loop,height=3cm]{16}{figures/gif_figs/CFRP_teflon_3o_375_375p_50kHz_5HC_x12_15Vpp/CFRP_teflon_30-}{1}{256}}\quad
			\subfloat[Intermidate ouputs]{\animategraphics[autoplay,loop,height=3cm]{15}{figures/gif_figs/CFRP_ijjeh_single_delamination/intermediate_output-}{0}{231}}\quad
			\subfloat[RMS]{\includegraphics[height=3cm,keepaspectratio]{figures/RMS_CFRP_teflon_3o_375_375p_50kHz_5HC_x12_15Vpp_Ijjeh_updated_results_.png}}\quad
			\subfloat[Binary RMS]{\includegraphics[height=3cm,keepaspectratio]{figures/Binary_RMS_CFRP_teflon_3o__375_375p_50kHz_5HC_x12_15Vpp_Ijjeh_.png}}
		\end{figure}			
	\end{alertblock}	
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\note{
	In this slide, I present the predicted results using the autoencoder ConvLSTM model regarding the single delamination case.
	
	Animation (a) shows the full wavefield measured by SLDV with 256 frames.
	
	Animation (b) shows the intermediate predictions of the model. 
	
	Figure (c) shows the RMS of all intermediate predictions.
	
	And finally, Figure (d) shows the binary RMS with IoU= 0.41		
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{subfigure}{0}	
\begin{frame}{Experimental results: Full wavefield based (Multiple delaminations)}
	\begin{columns}[T]
		\begin{column}[t]{0.20\textwidth}
			\begin{block}{Input}
				\footnotesize Full wavefield					
				\begin{figure}[ht!]	
					\centering						
					\subfloat{\animategraphics[autoplay,loop,height=0.32\textheight]{32}{figures/gif_figs/input_specimen_3/specimen_3-}{1}{512}}
				\end{figure}
			\end{block}				
		\end{column}
		\begin{column}[t]{0.40\textwidth}				
			\begin{block}{Adaptive wavenumber filtering}
				\footnotesize IoU$=0.04$					
				\begin{figure}[ht!]	
					\centering
					\subfloat{\includegraphics[height=0.32\textheight]{figures/mul/figure17a.png}}
					\quad
					\centering
					\subfloat{\includegraphics[height=0.32\textheight]{figures/mul/figure17b.png}}								
				\end{figure}
			\end{block}	
		\end{column}
		\begin{column}[t]{0.40\textwidth}				
			\begin{alertblock}{DL approach}	
				\footnotesize IoU= $0.64$						
				\begin{figure}[ht!]	
					\centering
					\subfloat{\includegraphics[height=0.32\textheight]{figures/RMS_L3_S3_B_333x333p_50kHz_5HC_18Vpp_x10_pzt_Ijjeh_updated_results_.png}}
					\quad
					\subfloat{\includegraphics[height=0.32\textheight]{figures/Binary_RMS_L3_S3_B__333x333p_50kHz_5HC_18Vpp_x10_pzt_Ijjeh_.png}}						
				\end{figure}				
			\end{alertblock}				
		\end{column}				
	\end{columns}	
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\note{
	Here, I present the predicted results using the autoencoder ConvLSTM model compared to adaptive wavenumber filtering for the multiple delamination case.
	The animation on the left shows the full wavefield measured by the SLDV of 512 frames.
	The figure shows the damage map resulting from applying adaptive wavenumber filtering, and this figure shows its binary output with IoU = 0.04
	Figure shows the RMS of all intermediate predictions, and figure shows the binary RMS with IoU = 0.64
	It can be concluded that utilising animations of Lamb waves propagation has better outcomes for delamination identification than the processing of a single image representing signal energy or RMS.		
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{subfigure}{0}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\subsection{Part II: Super-resolution image reconstruction}
%\begin{frame}{Super-resolution image reconstruction}
%	\begin{columns}[T]
%		\begin{column}[t]{0.6\textwidth}
%			\begin{alertblock}{Deep learning super-resolution model (DLSR)}					
%				\begin{footnotesize}
%					\justifying
%					\settowidth{\leftmargini}{\usebeamertemplate{itemize item}}
%					\addtolength{\leftmargini}{\labelsep}
%					\begin{itemize}
%						\item Registering HR full wavefield with an SLDV is a~time-consuming process.
%						\item DLSR model aims to recover HR full wavefield scans from a~LR measurements (below the Nyquist-Shannon sampling rate).
%					\end{itemize} 
%				\end{footnotesize}					
%			\end{alertblock}						
%			\begin{exampleblock}{Compressive sensing (CS) theory}
%				\footnotesize
%				\justifying
%				Any natural signal (\(x\)), e.g. (sounds, images) can be recovered using considerably fewer measurements (\(y\)) than standard methods.
%				\vfill
%				\begin{figure}[ht!]
%					\centering
%					\includegraphics[width=.45\textwidth]{matrix_mask.png}
%				\end{figure}
%			\end{exampleblock}							
%		\end{column}
%		\begin{column}[t]{0.4\textwidth}
%			\begin{figure}[ht!]
%				\centering
%				\includegraphics[width=1\textwidth]{superresolution_flowchart.png}
%			\end{figure}
%		\end{column}
%	\end{columns}		
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\note{
%	As mentioned earlier, the scanning laser Doppler vibrometer is a well-known non-contact tool for the acquisition of the full wavefield of propagating guided waves.
%	However, the process of acquiring the full wavefield of guided waves is time-consuming. 
%	One possible solution to tackle this problem is to acquire the Lamb waves at low-resolution grid points, and then the full wavefield can be reconstructed at high resolution.
%	Furthermore, I have compared my developed model of deep learning for super-resolution with the compressive sensing technique, which states that any signal can be reconstructed from a linear combination of random measurements.
%	To acquire the low-resolution measurements, I have used two types of masks, as shown in the figure: random and jitter.
%	The low-resolution measurements are far fewer than required by the Nyquist-Shannon sampling theory.
%}
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\setcounter{subfigure}{0}
%\begin{frame}{Evaluation metrics for DLSR model}
%	For evaluating the reconstructed HR full wavefield:
%	\begin{itemize}
%		\item Peak signal-to-noise ratio (PSNR):
%		\begin{equation*}
%			PSNR=20\log_{10}\left(\frac{R}{\sqrt{MSE}}\right)
%			\label{PSNR_}
%		\end{equation*}			
%		\item Pearson correlation coefficient (also known as Pearson's r):
%		\begin{equation*}
%			r_{xy} = \frac{\sum_{i=1}^{n}(x_i - \bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i - \bar{x})^2}\sqrt{\sum_{i=1}^{n}(y_i - \bar{y})^2}}
%			\label{Pearson_}
%		\end{equation*}
%	\end{itemize}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\note
%{
%	To evaluate the developed model, I used two metrics to measure the quality of the reconstructed high-resolution full wavefield frames:
%	\begin{itemize}
%		\item The first one is the peak signal-to-noise ratio (PSNR), which refers to the maximum possible power of a signal and the power of the distorting noise that affects the quality of its representation.
%		%			This equation depicts the mathematical representation of PSNR, where R is the maximum fluctuation value in the input image, and MSE is the mean squared error between the actual and predicted output.
%		%			
%		\item The second metric is the Pearson correlation coefficient (Pearson CC), which measures the linear relationship between two
%		variable sets \(X\) (represents the ground truth values) and \(Y\) (represents the predicted values) as shown in the below equation.
%		
%		%			Where \(n\) is the number of sample points,\(x_i, y_i\) are the individual value points representing the ground truth and predicted values, respectively, and \(\bar{x}\) and \(\bar{y}\) are the mean values of the sample and the prediction.
%	\end{itemize}
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\setcounter{subfigure}{0}
%\begin{frame}{Numerical test cases}
%	\begin{columns}[T]				
%		\begin{column}[c]{0.5\textwidth}				
%			\begin{figure}
%				\centering
%				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%				%					\only<1>{
%					%						\begin{alertblock}{First test case}
%						%							\begin{figure}
%							%								\includegraphics[height=.35\textheight]{LR_456_frame_159_input.png}
%							%								\caption{LR input, $f_n=159$}
%							%							\end{figure}							
%						%							%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%						%							\begin{table}[!h]
%							%								\centering 
%							%								\footnotesize
%							%								\begin{tabular}{cccc}
%								%									\toprule
%								%									\multicolumn{2}{c}{plate} & \multicolumn{2}{c}{delamination} \\
%								%									\cmidrule(lr){1-2} \cmidrule(lr){3-4}
%								%									PSNR & PEARSON CC & PSNR & PEARSON CC \\ 
%								%									\midrule
%								%									42.95 & 0.999 & 33.02 & 0.993 \\					
%								%									\bottomrule
%								%								\end{tabular}
%							%							\end{table}
%						%						\end{alertblock}}
%				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%				\only<1>{
%					\begin{alertblock}{First test case}
%						\begin{figure}
%							\includegraphics[height=.35\textheight]{LR_438_frame_154_input.png}
%							\caption{LR input, $f_n=154$}
%						\end{figure}							
%						%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%						\begin{table}[!h]
%							\centering 
%							\footnotesize
%							\begin{tabular}{cccc}
%								\toprule
%								\multicolumn{2}{c}{plate} & \multicolumn{2}{c}{delamination} \\
%								\cmidrule(lr){1-2} \cmidrule(lr){3-4}
%								PSNR & PEARSON CC & PSNR & PEARSON CC \\ 
%								\midrule
%								47.00 & 0.998 & 38.52 & 0.995 \\					
%								\bottomrule
%							\end{tabular}
%							\label{tab:num_DLSR_results_2_}
%						\end{table}	
%						%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%				\end{alertblock}}
%				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%				\only<2>{
%					\begin{alertblock}{Second test case}
%						\begin{figure}
%							\includegraphics[height=.35\textheight]{LR_397_frame_127_input.png}
%							\caption{LR input, $f_n=127$}
%						\end{figure}						
%						%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%						\begin{table}[!h]
%							\centering 
%							\footnotesize	
%							\begin{tabular}{cccc}
%								\toprule
%								\multicolumn{2}{c}{plate} & \multicolumn{2}{c}{delamination} \\
%								\cmidrule(lr){1-2} \cmidrule(lr){3-4}
%								PSNR & PEARSON CC & PSNR & PEARSON CC \\ 
%								\midrule
%								48.60 & 0.998 & 46.67 & 0.998 \\					
%								\bottomrule
%							\end{tabular}
%						\end{table}
%						%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%				\end{alertblock}}
%				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%			\end{figure}
%		\end{column}
%		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%		\begin{column}[c]{0.5\textwidth}
%			%				\only<1>{	
%				%					\setcounter{subfigure}{0}
%				%					\begin{figure}
%					%						\subfloat[listentry][HR ref]{\includegraphics[height=.35\textheight]{output_456_frame_159_full_frame_GT.png}}\quad
%					%						\subfloat[listentry][DLSR]{\includegraphics[height=.35\textheight]{output_456_frame_159_full_frame_pred.png}}
%					%						\\
%					%						\subfloat[listentry][Ref]{\includegraphics[height=.35\textheight]{output_456_frame_159_delamination_GT.png}}\quad
%					%						\subfloat[listentry][DLSR]{\includegraphics[height=.35\textheight]{output_456_frame_159_delamination_pred.png}}
%					%					\end{figure}}			
%			\only<1>{
%				\setcounter{subfigure}{0}					
%				%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%				\begin{figure}
%					\subfloat[listentry][HR ref]{\includegraphics[height=.35\textheight]{output_438_frame_154_full_frame_GT.png}}\quad
%					\subfloat[listentry][DLSR]{\includegraphics[height=.35\textheight]{output_438_frame_154_full_frame_pred.png}}
%					\\
%					\subfloat[listentry][Ref]{\includegraphics[height=.35\textheight]{output_438_frame_154_delamination_GT.png}}\quad	
%					\subfloat[listentry][DLSR]{\includegraphics[height=.35\textheight]{output_438_frame_154_delamination_pred.png}}
%			\end{figure}}
%			\only<2>{	
%				\setcounter{subfigure}{0}
%				\begin{figure}
%					\centering
%					\subfloat[listentry][HR ref]{\includegraphics[height=.35\textheight]{output_397_frame_127_full_frame_GT.png}}\quad
%					\subfloat[listentry][DLSR]{\includegraphics[height=.35\textheight]{output_397_frame_127_full_frame_pred.png}}
%					\\
%					\subfloat[listentry][Ref]{\includegraphics[height=.35\textheight]{output_397_frame_127_delamination_GT.png}}\quad
%					\subfloat[listentry][DLSR]{\includegraphics[height=.35\textheight]{output_397_frame_127_delamination_pred.png}}
%			\end{figure}}
%		\end{column}
%	\end{columns}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\note
%{
%	\footnotesize
%	In the following, the results of the reconstruction of HR frames for two numerical test cases will be presented.				
%	
%	In the first test case, the figure shows the low-resolution measurements at frame number 154.		
%	Figure a shows the actual HR frame, and figure b shows the predicted SR frame. 
%	The PSNR value is 47
%	
%	Figure c shows the HR sub-frame at the delamination region, figure d shows the SR prediction at the delamination region, and the PSNR is 38.52.
%	
%	In the second test case, the figure shows the low-resolution measurements at frame number 127.
%	Figure a shows the actual HR frame, and figure b shows the predicted SR frame. 
%	The PSNR value is 42.95
%	
%	Figure c shows the HR sub-frame at the delamination region, figure d shows the SR prediction at the delamination region, and the PSNR is 33.02.
%	
%	
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\setcounter{subfigure}{0}
%\begin{frame}{Experimental test case}		
%	\begin{columns}[T]
%		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%		\begin{column}[t]{0.25\textwidth}				
%			\begin{figure}	
%				\centering					
%				\includegraphics[width=1\textwidth]{frame110_32x32.png}
%				%					\caption{LR input \((N_f = 110)\)}
%			\end{figure}
%			\footnotesize
%			LR measurements (Input): \(32\times32=1024\)p. \\
%			HR (Output): \(512\times512=262144\)p.
%		\end{column}
%		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%		\begin{column}[t]{.25\textwidth}
%			\begin{block}{HR label}
%				\begin{figure}
%					\centering
%					\subfloat{\includegraphics[width=0.75\textwidth]{figure10a.png}}
%					\vfill
%					\subfloat{\includegraphics[width=0.75\textwidth]{figure11a.png}}
%				\end{figure}
%			\end{block}
%		\end{column}
%		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%		\begin{column}[t]{.25\textwidth}
%			\begin{block}{CS: 1024p}
%				\begin{figure}
%					\centering
%					\subfloat{\includegraphics[width=0.75\textwidth]{figure10b.png}}
%					\vfill						
%					\subfloat{\includegraphics[width=0.75\textwidth]{figure11b.png}}
%				\end{figure}
%			\end{block}				
%		\end{column}
%		%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%		\begin{column}[t]{.25\textwidth}
%			\begin{alertblock}{DLSR}
%				\begin{figure}
%					\centering
%					\subfloat{\includegraphics[width=0.75\textwidth]{figure10e.png}}
%					\vfill			
%					\subfloat{\includegraphics[width=0.75\textwidth]{figure11e.png}}\quad
%				\end{figure}
%			\end{alertblock}				
%			%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%		\end{column}				
%	\end{columns}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\note{
%	\footnotesize
%	In this slide, I present an experimental test case,
%	
%	The figure on the left represents an experimentally acquired full wavefield in it low resolution.	
%	
%	This Figure shows the actual HR reference frame for the whole plate and at the delamination area
%	
%	The figure in the middle represents the outputs of applying the compressive sensing technique in which it shows a poor quality of reconstruction. 
%	
%	
%	The figure on the right presents the reconstructed HR frame with the DLSR model for the whole plate and at the delamination area as you can see the quality of reconstruction is very noticeable. 
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\begin{frame}{Analysis of experimental case}
%	\begin{table}[!ht]
%		\renewcommand{\arraystretch}{1.3}
%		\centering \footnotesize
%		\caption{Quality metrics for tested methods.}	
%		\begin{tabular}{lrrrcrc} 
%			\toprule[1.5pt]
%			& & & \multicolumn{2}{c}{plate} & \multicolumn{2}{c}{delamination} \\
%			\cmidrule(lr){4-5} \cmidrule(lr){6-7}
%			Method & $N_p$ & CR [\%] & PSNR & PEARSON CC& PSNR & PEARSON CC \\
%			\midrule
%			\csvreader
%			[table head=\toprule,
%			late after line=\\ 
%			]{table_metrics.csv}{
%				1=\one, 2=\two, 3=\three, 4=\four, 5=\five, 6=\six, 7=\seven
%			}%
%			{\one & \two & \three & \four & \five & \six & \seven }%	
%			\bottomrule[1.5pt]
%		\end{tabular}	
%		\label{tab:csv_results_}
%	\end{table}
%\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\note{
%	The following table presents a detailed comparison of the quality metrics for CS methods with applied jitter and random masks and the DLSR model.
%	As shown in the table the DLSR model achieved the highest PSNR and Pearson CC values.		
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Conclusions}		
		\begin{justify}
			\settowidth{\leftmargini}{\usebeamertemplate{itemize item}}
			\addtolength{\leftmargini}{\labelsep}
			\begin{itemize}
				\item{Improved Damage Detection}			
				\item{Enhanced Fault Diagnosis}	
				\item{Real-time Monitoring and Early Warning}					
				\item{Reduced Manual Effort and Cost}
				\item{Increased Accuracy and Reliability}
				\item{Adaptability and Generalization}
				\item{Data-Driven Decision Making}
			\end{itemize}
		\end{justify}									
\end{frame}			
\note{
	Utilizing artificial intelligence (AI) and deep learning approaches in structural health monitoring (SHM) has several significant benefits and conclusions. Here are some key conclusions of applying AI and deep learning in SHM:
	
	Improved Damage Detection: AI and deep learning techniques have shown promise in accurately detecting and localizing damage in structures. They can learn complex patterns and features from sensor data, enabling more effective damage identification compared to traditional methods.
	
	Enhanced Fault Diagnosis: AI-based approaches enable automated and intelligent fault diagnosis in structures. By analyzing sensor data, these techniques can identify specific faults or anomalies, allowing for timely maintenance and repairs.
	
	Real-time Monitoring and Early Warning: AI and deep learning algorithms can process large amounts of sensor data in real-time, facilitating continuous monitoring of structural health. This capability enables early warning systems, alerting operators or stakeholders about potential structural issues before they escalate into critical failures.
	
	Reduced Manual Effort and Cost: By automating the analysis and interpretation of sensor data, AI and deep learning techniques reduce the need for extensive manual labor and costly manual inspections. This efficiency leads to cost savings in terms of maintenance and monitoring efforts.
	
	Increased Accuracy and Reliability: Deep learning models can capture intricate relationships in complex structural data, resulting in improved accuracy and reliability in SHM tasks. They can handle high-dimensional data and handle noise or variability, leading to more robust and dependable structural health assessments.
	
	Adaptability and Generalization: AI and deep learning approaches have the capability to adapt to different types of structures and environments. They can generalize well across various structural systems, allowing for transfer learning and application to different scenarios without requiring significant retraining.
	
	Data-Driven Decision Making: AI and deep learning techniques enable data-driven decision making in SHM. By analyzing historical data and real-time sensor information, these approaches can provide insights and recommendations for effective structural maintenance and management strategies.
	
	Overall, the utilization of AI and deep learning in structural health monitoring offers significant advancements in damage detection, fault diagnosis, real-time monitoring, cost savings, accuracy, adaptability, and data-driven decision making. These approaches have the potential to revolutionize the field of SHM and contribute to safer and more efficient management of structures.
}	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{subfigure}{0}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
{
	\setbeamercolor{palette primary}{fg=blue, bg=white}
	\begin{frame}[standout]
		\centering
		Thank you for your listening!\\ \vspace{12pt}
		Questions?\\ \vspace{12pt}
		\url{abdalraheem.ijjeh@gmail.com}
	\end{frame}
}
\note{}	
%	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}